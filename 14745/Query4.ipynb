{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8df66e2d-359d-4793-82f6-bf04283d8b13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1942</td><td>application_1732639283265_1903</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1903/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-112.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1903_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+------------+\n",
      "|Vict Descent Full             |Victim_Count|\n",
      "+------------------------------+------------+\n",
      "|White                         |649         |\n",
      "|Other                         |72          |\n",
      "|Hispanic/Latin/Mexican        |66          |\n",
      "|Unknown                       |38          |\n",
      "|Black                         |37          |\n",
      "|Other Asian                   |21          |\n",
      "|Chinese                       |1           |\n",
      "|American Indian/Alaskan Native|1           |\n",
      "+------------------------------+------------+\n",
      "\n",
      "+------------------------------+------------+\n",
      "|Vict Descent Full             |Victim_Count|\n",
      "+------------------------------+------------+\n",
      "|Hispanic/Latin/Mexican        |2815        |\n",
      "|Black                         |761         |\n",
      "|White                         |330         |\n",
      "|Other                         |187         |\n",
      "|Other Asian                   |113         |\n",
      "|Unknown                       |22          |\n",
      "|American Indian/Alaskan Native|21          |\n",
      "|Korean                        |5           |\n",
      "|Chinese                       |3           |\n",
      "|AsianIndian                   |1           |\n",
      "|Filipino                      |1           |\n",
      "+------------------------------+------------+\n",
      "\n",
      "Time taken: 98.08 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from sedona.register.geo_registrator import SedonaRegistrator\n",
    "from sedona.spark import SedonaContext\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "# Initialize Spark Session and Sedona Context\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CrimeAnalysisSpatialJoinSQL1\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.sedona.sql.SedonaSqlExtensions\") \\\n",
    "    .config(\"sedona.global.charset\", \"utf8\") \\\n",
    "    .config(\"spark.executor.instances\", 2)   \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", 1)  \\\n",
    "    .getOrCreate()\n",
    "    # .config(\"spark.driver.memory\", \"4g\") \\\n",
    "\n",
    "\n",
    "sedona = SedonaContext.create(spark)\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "# Load datasets\n",
    "census_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "income_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\"\n",
    "crime_path = \"s3://groups-bucket-dblab-905418150721/group24/results/q2_parquet_maindata/\"\n",
    "\n",
    "# Load Census Data (GeoJSON format)\n",
    "census_df = sedona.read.format(\"geojson\") \\\n",
    "    .option(\"multiLine\", \"true\").load(census_path) \\\n",
    "    .selectExpr(\"explode(features) as features\") \\\n",
    "    .select(\"features.*\")\n",
    "\n",
    "# Flatten the GeoJSON structure and filter valid populations\n",
    "census_df = census_df.select(\n",
    "    F.col(\"properties.ZCTA10\").alias(\"ZCTA10\"),\n",
    "    F.col(\"properties.POP_2010\").alias(\"Population\"),\n",
    "    F.col(\"properties.COMM\").alias(\"COMM\"),\n",
    "    F.col(\"geometry\").alias(\"geometry\"),\n",
    "    F.col(\"properties.HOUSING10\").alias(\"HOUSING10\"),\n",
    "    F.col(\"properties.CITY\").alias(\"CITY\"),\n",
    "# ).filter(F.col(\"Population\") > 0)\n",
    ").filter((F.col(\"CITY\") == \"Los Angeles\") & (F.col(\"Population\") > 0) & (F.col(\"HOUSING10\") > 0))\n",
    "# Load Income Data (CSV format)\n",
    "income_df = spark.read.csv(income_path, header=True, inferSchema=True)\n",
    "income_df = income_df.withColumn(\n",
    "    \"Income\",\n",
    "    F.regexp_replace(F.col(\"Estimated Median Income\"), \"[$,]\", \"\").cast(\"double\")\n",
    ").withColumnRenamed(\"Zip Code\",\"Zip\").drop(\"Estimated Median Income\")\n",
    "\n",
    "# Calculate total income per community (sum of income contributions per zip)\n",
    "income_total = census_df.join(income_df, census_df.ZCTA10 == income_df.Zip, \"inner\") \\\n",
    "    .groupBy(\"COMM\").agg(\n",
    "        F.sum(F.col(\"Income\") * F.col(\"HOUSING10\")).alias(\"Total_Income\")\n",
    "    )\n",
    "\n",
    "# Aggregate census data by community\n",
    "census_agg = census_df.groupBy(\"COMM\").agg(\n",
    "    F.sum(\"Population\").alias(\"Total_Population\"),\n",
    "    F.sum(\"HOUSING10\").alias(\"Total_Households\")\n",
    ")\n",
    "\n",
    "# Join census and income data\n",
    "census_income = census_agg.join(income_total, \"COMM\", \"inner\")\n",
    "\n",
    "# Calculate Mean Income Per Person\n",
    "census_income = census_income.withColumn(\n",
    "    \"Mean_Income_Per_Person\",\n",
    "    F.col(\"Total_Income\") / F.col(\"Total_Population\")\n",
    ")\n",
    "\n",
    "# Step 1: Get the top 3 and bottom 3 communities by Mean Income Per Person\n",
    "top_communities = census_income.orderBy(F.col(\"Mean_Income_Per_Person\"), ascending=False).limit(3)\n",
    "bottom_communities = census_income.orderBy(F.col(\"Mean_Income_Per_Person\"), ascending=True).limit(3)\n",
    "\n",
    "# Step 2: Filter crime data for top and bottom communities\n",
    "top_community_codes = [row['COMM'] for row in top_communities.collect()]\n",
    "bottom_community_codes = [row['COMM'] for row in bottom_communities.collect()]\n",
    "\n",
    "# Load Crime Data (Parquet format)\n",
    "crime_df = spark.read.parquet(crime_path)\n",
    "# Convert DATE OCC to a timestamp format\n",
    "crime_df = crime_df.withColumn(\"DATE_OCC_TS\", F.to_timestamp(\"DATE OCC\", \"MM/dd/yyyy hh:mm:ss a\"))\n",
    "\n",
    "# Filter rows where the year is 2015\n",
    "crime_df = crime_df.filter(F.year(\"DATE_OCC_TS\") == 2015)\n",
    "\n",
    "# Create geometry column using ST_Point\n",
    "crime_df = crime_df.withColumn(\"geometry\", F.expr(\"ST_Point(LON, LAT)\"))\n",
    "\n",
    "# Step 3: Join the crime data with census data to include 'COMM' (Community) info\n",
    "crime_with_comm_df = crime_df.alias(\"cr\").join(\n",
    "    census_df.alias(\"c\"),\n",
    "    F.expr(\"ST_Within(cr.geometry, c.geometry)\"),\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "# Filter crime data for top communities (those with highest income)\n",
    "top_crime_df = crime_with_comm_df.filter(F.col(\"COMM\").isin(top_community_codes))\n",
    "\n",
    "# Filter crime data for bottom communities (those with lowest income)\n",
    "bottom_crime_df = crime_with_comm_df.filter(F.col(\"COMM\").isin(bottom_community_codes))\n",
    "\n",
    "# Step 4: Load race codes dataset to translate 'Vict Descent'\n",
    "race_codes_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/RE_codes.csv\"\n",
    "race_codes_df = spark.read.csv(race_codes_path, header=True, inferSchema=True)\n",
    "\n",
    "# Join the crime data with the race codes to get the full description of 'Vict Descent'\n",
    "top_crime_with_race_df = top_crime_df.join(race_codes_df, top_crime_df[\"Vict Descent\"] == race_codes_df[\"Vict Descent\"]) #, \"left_outer\")\n",
    "\n",
    "# Aggregate by 'Vict Descent Full' and count victims for top communities\n",
    "top_crime_race_count = top_crime_with_race_df.groupBy(\"Vict Descent Full\").agg(\n",
    "    F.count(\"*\").alias(\"Victim_Count\")\n",
    ").orderBy(F.col(\"Victim_Count\"), ascending=False)\n",
    "\n",
    "# Join the crime data with the race codes for bottom communities\n",
    "bottom_crime_with_race_df = bottom_crime_df.join(race_codes_df, bottom_crime_df[\"Vict Descent\"] == race_codes_df[\"Vict Descent\"]) #, \"left_outer\")\n",
    "\n",
    "# Aggregate by 'Vict Descent Full' and count victims for bottom communities\n",
    "bottom_crime_race_count = bottom_crime_with_race_df.groupBy(\"Vict Descent Full\").agg(\n",
    "    F.count(\"*\").alias(\"Victim_Count\")\n",
    ").orderBy(F.col(\"Victim_Count\"), ascending=False)\n",
    "\n",
    "\n",
    "top_crime_race_count.show(truncate=False)\n",
    "bottom_crime_race_count.show(truncate=False)\n",
    "\n",
    "# Stop timing and print out the execution duration\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65b80e5a-f79c-4353-9249-3c5f412f0c17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+----+\n",
      "|Vict Descent                  |#   |\n",
      "+------------------------------+----+\n",
      "|White                         |5726|\n",
      "|Unknown                       |836 |\n",
      "|Other                         |690 |\n",
      "|Hispanic/Latin/Mexican        |558 |\n",
      "|Black                         |260 |\n",
      "|Other Asian                   |204 |\n",
      "|Chinese                       |29  |\n",
      "|Japanese                      |20  |\n",
      "|Filipino                      |16  |\n",
      "|Korean                        |13  |\n",
      "|AsianIndian                   |5   |\n",
      "|American Indian/Alaskan Native|4   |\n",
      "|Vietnamese                    |3   |\n",
      "|Hawaiian                      |1   |\n",
      "+------------------------------+----+\n",
      "\n",
      "+------------------------------+-----+\n",
      "|Vict Descent                  |#    |\n",
      "+------------------------------+-----+\n",
      "|Hispanic/Latin/Mexican        |19703|\n",
      "|Black                         |6316 |\n",
      "|Unknown                       |5077 |\n",
      "|White                         |1469 |\n",
      "|Other                         |807  |\n",
      "|Other Asian                   |166  |\n",
      "|Filipino                      |12   |\n",
      "|Japanese                      |8    |\n",
      "|Korean                        |8    |\n",
      "|Laotian                       |5    |\n",
      "|Chinese                       |4    |\n",
      "|American Indian/Alaskan Native|4    |\n",
      "|Cambodian                     |3    |\n",
      "|Vietnamese                    |3    |\n",
      "|Pacific Islander              |3    |\n",
      "|Guamanian                     |2    |\n",
      "|Hawaiian                      |1    |\n",
      "+------------------------------+-----+\n",
      "\n",
      "Time taken: 78.48 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from sedona.register.geo_registrator import SedonaRegistrator\n",
    "from sedona.spark import SedonaContext\n",
    "from pyspark.sql.types import StringType\n",
    "import time\n",
    "\n",
    "# Initialize Spark Session and Sedona Context\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CrimeAnalysisSpatialJoinSQL2\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.sedona.sql.SedonaSqlExtensions\") \\\n",
    "    .config(\"sedona.global.charset\", \"utf8\") \\\n",
    "    .config(\"spark.executor.instances\", 2)   \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", 2)  \\\n",
    "    .getOrCreate()\n",
    "    # .config(\"spark.driver.memory\", \"8g\") \\\n",
    "\n",
    "\n",
    "sedona = SedonaContext.create(spark)\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "# Load datasets\n",
    "census_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "income_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\"\n",
    "crime_path = \"s3://groups-bucket-dblab-905418150721/group24/results/q2_parquet_maindata/\"\n",
    "\n",
    "# Load Census Data (GeoJSON format)\n",
    "census_df = sedona.read.format(\"geojson\") \\\n",
    "    .option(\"multiLine\", \"true\").load(census_path) \\\n",
    "    .selectExpr(\"explode(features) as features\") \\\n",
    "    .select(\"features.*\")\n",
    "\n",
    "# Flatten the GeoJSON structure and filter valid populations\n",
    "census_df = census_df.select(\n",
    "    F.col(\"properties.ZCTA10\").alias(\"ZCTA10\"),\n",
    "    F.col(\"properties.POP_2010\").alias(\"Population\"),\n",
    "    F.col(\"properties.COMM\").alias(\"COMM\"),\n",
    "    F.col(\"geometry\").alias(\"geometry\"),\n",
    "    F.col(\"properties.HOUSING10\").alias(\"HOUSING10\"),\n",
    ").filter(F.col(\"Population\") > 0)  # Exclude zero or negative population\n",
    "\n",
    "# Load Income Data (CSV format)\n",
    "income_df = spark.read.csv(income_path, header=True, inferSchema=True)\n",
    "income_df = income_df.withColumn(\n",
    "    \"Income\",\n",
    "    F.regexp_replace(F.col(\"Estimated Median Income\"), \"[$,]\", \"\").cast(\"double\")\n",
    ").withColumnRenamed(\"Zip Code\",\"Zip\").drop(\"Estimated Median Income\")\n",
    "\n",
    "# Calculate total income per community (sum of income contributions per zip)\n",
    "income_total = census_df.join(income_df, census_df.ZCTA10 == income_df.Zip, \"inner\") \\\n",
    "    .groupBy(\"COMM\").agg(\n",
    "        F.sum(F.col(\"Income\") * F.col(\"HOUSING10\")).alias(\"Total_Income\")\n",
    "    )\n",
    "\n",
    "# Aggregate census data by community\n",
    "census_agg = census_df.groupBy(\"COMM\").agg(\n",
    "    F.sum(\"Population\").alias(\"Total_Population\"),\n",
    "    F.sum(\"HOUSING10\").alias(\"Total_Households\")\n",
    ")\n",
    "\n",
    "# Join census and income data\n",
    "census_income = census_agg.join(income_total, \"COMM\", \"inner\")\n",
    "\n",
    "# Calculate Mean Income Per Person\n",
    "census_income = census_income.withColumn(\n",
    "    \"Mean_Income_Per_Person\",\n",
    "    F.col(\"Total_Income\") / F.col(\"Total_Population\")\n",
    ")\n",
    "\n",
    "# Step 1: Get the top 3 and bottom 3 communities by Mean Income Per Person\n",
    "top_communities = census_income.orderBy(F.col(\"Mean_Income_Per_Person\"), ascending=False).limit(3)\n",
    "bottom_communities = census_income.orderBy(F.col(\"Mean_Income_Per_Person\"), ascending=True).limit(3)\n",
    "\n",
    "# Step 2: Filter crime data for top and bottom communities\n",
    "top_community_codes = [row['COMM'] for row in top_communities.collect()]\n",
    "bottom_community_codes = [row['COMM'] for row in bottom_communities.collect()]\n",
    "\n",
    "# Load Crime Data (Parquet format)\n",
    "crime_df = spark.read.parquet(crime_path)\n",
    "\n",
    "# Create geometry column using ST_Point\n",
    "crime_df = crime_df.withColumn(\"geometry\", F.expr(\"ST_Point(LON, LAT)\"))\n",
    "\n",
    "# Step 3: Join the crime data with census data to include 'COMM' (Community) info\n",
    "crime_with_comm_df = crime_df.alias(\"cr\").join(\n",
    "    census_df.alias(\"c\"),\n",
    "    F.expr(\"ST_Within(cr.geometry, c.geometry)\"),\n",
    "    \"inner\"\n",
    ")\n",
    "# .select(\n",
    "#     \"census_df.COMM\"\n",
    "# )\n",
    "\n",
    "# Filter crime data for top communities (those with highest income)\n",
    "top_crime_df = crime_with_comm_df.filter(F.col(\"COMM\").isin(top_community_codes))\n",
    "\n",
    "# Filter crime data for bottom communities (those with lowest income)\n",
    "bottom_crime_df = crime_with_comm_df.filter(F.col(\"COMM\").isin(bottom_community_codes))\n",
    "\n",
    "# Step 4: Load race codes dataset to translate 'Vict Descent'\n",
    "race_codes_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/RE_codes.csv\"\n",
    "race_codes_df = spark.read.csv(race_codes_path, header=True, inferSchema=True)\n",
    "\n",
    "# Join the crime data with the race codes to get the full description of 'Vict Descent'\n",
    "top_crime_with_race_df = top_crime_df.join(race_codes_df, top_crime_df[\"Vict Descent\"] == race_codes_df[\"Vict Descent\"], \"left_outer\")\n",
    "\n",
    "# Aggregate by 'Vict Descent Full' and count victims for top communities\n",
    "top_crime_race_count = top_crime_with_race_df.groupBy(\"Vict Descent Full\").agg(\n",
    "    F.count(\"*\").alias(\"Victim_Count\")\n",
    ").orderBy(F.col(\"Victim_Count\"), ascending=False)\n",
    "\n",
    "# Join the crime data with the race codes for bottom communities\n",
    "bottom_crime_with_race_df = bottom_crime_df.join(race_codes_df, bottom_crime_df[\"Vict Descent\"] == race_codes_df[\"Vict Descent\"], \"left_outer\")\n",
    "\n",
    "# Aggregate by 'Vict Descent Full' and count victims for bottom communities\n",
    "bottom_crime_race_count = bottom_crime_with_race_df.groupBy(\"Vict Descent Full\").agg(\n",
    "    F.count(\"*\").alias(\"Victim_Count\")\n",
    ").orderBy(F.col(\"Victim_Count\"), ascending=False)\n",
    "\n",
    "# Clean up NULL values by adding them to the existing 'Unknown' category\n",
    "top_crime_race_count_cleaned = top_crime_race_count.groupBy(\n",
    "    F.when(F.col(\"Vict Descent Full\").isNull(), \"Unknown\")\n",
    "    .otherwise(F.col(\"Vict Descent Full\")).alias(\"Vict Descent\")\n",
    ").agg(\n",
    "    F.sum(\"Victim_Count\").alias(\"#\")\n",
    ").orderBy(F.col(\"#\"), ascending=False)\n",
    "\n",
    "bottom_crime_race_count_cleaned = bottom_crime_race_count.groupBy(\n",
    "    F.when(F.col(\"Vict Descent Full\").isNull(), \"Unknown\")\n",
    "    .otherwise(F.col(\"Vict Descent Full\")).alias(\"Vict Descent\")\n",
    ").agg(\n",
    "    F.sum(\"Victim_Count\").alias(\"#\")\n",
    ").orderBy(F.col(\"#\"), ascending=False)\n",
    "\n",
    "\n",
    "# You can now show the cleaned-up DataFrames or export them\n",
    "top_crime_race_count_cleaned.show(truncate=False)\n",
    "bottom_crime_race_count_cleaned.show(truncate=False)\n",
    "\n",
    "# Stop timing and print out the execution duration\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fcd64da-f8eb-4f89-a069-9ce8079d1963",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+------------+\n",
      "|Vict Descent Full             |Victim_Count|\n",
      "+------------------------------+------------+\n",
      "|White                         |5726        |\n",
      "|Unknown                       |836         |\n",
      "|Other                         |690         |\n",
      "|Hispanic/Latin/Mexican        |558         |\n",
      "|Black                         |260         |\n",
      "|Other Asian                   |204         |\n",
      "|Chinese                       |29          |\n",
      "|Japanese                      |20          |\n",
      "|Filipino                      |16          |\n",
      "|Korean                        |13          |\n",
      "|AsianIndian                   |5           |\n",
      "|American Indian/Alaskan Native|4           |\n",
      "|Vietnamese                    |3           |\n",
      "|Hawaiian                      |1           |\n",
      "+------------------------------+------------+\n",
      "\n",
      "+------------------------------+------------+\n",
      "|Vict Descent Full             |Victim_Count|\n",
      "+------------------------------+------------+\n",
      "|Hispanic/Latin/Mexican        |19703       |\n",
      "|Black                         |6316        |\n",
      "|Unknown                       |5077        |\n",
      "|White                         |1469        |\n",
      "|Other                         |807         |\n",
      "|Other Asian                   |166         |\n",
      "|Filipino                      |12          |\n",
      "|Japanese                      |8           |\n",
      "|Korean                        |8           |\n",
      "|Laotian                       |5           |\n",
      "|Chinese                       |4           |\n",
      "|American Indian/Alaskan Native|4           |\n",
      "|Cambodian                     |3           |\n",
      "|Vietnamese                    |3           |\n",
      "|Pacific Islander              |3           |\n",
      "|Guamanian                     |2           |\n",
      "|Hawaiian                      |1           |\n",
      "+------------------------------+------------+\n",
      "\n",
      "Time taken: 67.92 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from sedona.register.geo_registrator import SedonaRegistrator\n",
    "from sedona.spark import SedonaContext\n",
    "from pyspark.sql.types import StringType\n",
    "import time\n",
    "\n",
    "# Initialize Spark Session and Sedona Context\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CrimeAnalysisSpatialJoinSQL3\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.sedona.sql.SedonaSqlExtensions\") \\\n",
    "    .config(\"sedona.global.charset\", \"utf8\") \\\n",
    "    .config(\"spark.executor.instances\", 2)   \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.cores\", 4)  \\\n",
    "    .getOrCreate()\n",
    "\n",
    "    # .config(\"spark.driver.memory\", \"16g\") \\\n",
    "\n",
    "sedona = SedonaContext.create(spark)\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "# Load datasets\n",
    "census_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "income_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\"\n",
    "crime_path = \"s3://groups-bucket-dblab-905418150721/group24/results/q2_parquet_maindata/\"\n",
    "\n",
    "# Load Census Data (GeoJSON format)\n",
    "census_df = sedona.read.format(\"geojson\") \\\n",
    "    .option(\"multiLine\", \"true\").load(census_path) \\\n",
    "    .selectExpr(\"explode(features) as features\") \\\n",
    "    .select(\"features.*\")\n",
    "\n",
    "# Flatten the GeoJSON structure and filter valid populations\n",
    "census_df = census_df.select(\n",
    "    F.col(\"properties.ZCTA10\").alias(\"ZCTA10\"),\n",
    "    F.col(\"properties.POP_2010\").alias(\"Population\"),\n",
    "    F.col(\"properties.COMM\").alias(\"COMM\"),\n",
    "    F.col(\"geometry\").alias(\"geometry\"),\n",
    "    F.col(\"properties.HOUSING10\").alias(\"HOUSING10\"),\n",
    ").filter(F.col(\"Population\") > 0)  # Exclude zero or negative population\n",
    "\n",
    "# Load Income Data (CSV format)\n",
    "income_df = spark.read.csv(income_path, header=True, inferSchema=True)\n",
    "income_df = income_df.withColumn(\n",
    "    \"Income\",\n",
    "    F.regexp_replace(F.col(\"Estimated Median Income\"), \"[$,]\", \"\").cast(\"double\")\n",
    ").withColumnRenamed(\"Zip Code\",\"Zip\").drop(\"Estimated Median Income\")\n",
    "\n",
    "# Calculate total income per community (sum of income contributions per zip)\n",
    "income_total = census_df.join(income_df, census_df.ZCTA10 == income_df.Zip, \"inner\") \\\n",
    "    .groupBy(\"COMM\").agg(\n",
    "        F.sum(F.col(\"Income\") * F.col(\"HOUSING10\")).alias(\"Total_Income\")\n",
    "    )\n",
    "\n",
    "# Aggregate census data by community\n",
    "census_agg = census_df.groupBy(\"COMM\").agg(\n",
    "    F.sum(\"Population\").alias(\"Total_Population\"),\n",
    "    F.sum(\"HOUSING10\").alias(\"Total_Households\")\n",
    ")\n",
    "\n",
    "# Join census and income data\n",
    "census_income = census_agg.join(income_total, \"COMM\", \"inner\")\n",
    "\n",
    "# Calculate Mean Income Per Person\n",
    "census_income = census_income.withColumn(\n",
    "    \"Mean_Income_Per_Person\",\n",
    "    F.col(\"Total_Income\") / F.col(\"Total_Population\")\n",
    ")\n",
    "\n",
    "# Step 1: Get the top 3 and bottom 3 communities by Mean Income Per Person\n",
    "top_communities = census_income.orderBy(F.col(\"Mean_Income_Per_Person\"), ascending=False).limit(3)\n",
    "bottom_communities = census_income.orderBy(F.col(\"Mean_Income_Per_Person\"), ascending=True).limit(3)\n",
    "\n",
    "# Step 2: Filter crime data for top and bottom communities\n",
    "top_community_codes = [row['COMM'] for row in top_communities.collect()]\n",
    "bottom_community_codes = [row['COMM'] for row in bottom_communities.collect()]\n",
    "\n",
    "# Load Crime Data (Parquet format)\n",
    "crime_df = spark.read.parquet(crime_path)\n",
    "\n",
    "# Create geometry column using ST_Point\n",
    "crime_df = crime_df.withColumn(\"geometry\", F.expr(\"ST_Point(LON, LAT)\"))\n",
    "\n",
    "# Step 3: Join the crime data with census data to include 'COMM' (Community) info\n",
    "crime_with_comm_df = crime_df.alias(\"cr\").join(\n",
    "    census_df.alias(\"c\"),\n",
    "    F.expr(\"ST_Within(cr.geometry, c.geometry)\"),\n",
    "    \"inner\"\n",
    ")\n",
    "# .select(\n",
    "#     \"census_df.COMM\"\n",
    "# )\n",
    "\n",
    "# Filter crime data for top communities (those with highest income)\n",
    "top_crime_df = crime_with_comm_df.filter(F.col(\"COMM\").isin(top_community_codes))\n",
    "\n",
    "# Filter crime data for bottom communities (those with lowest income)\n",
    "bottom_crime_df = crime_with_comm_df.filter(F.col(\"COMM\").isin(bottom_community_codes))\n",
    "\n",
    "# Step 4: Load race codes dataset to translate 'Vict Descent'\n",
    "race_codes_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/RE_codes.csv\"\n",
    "race_codes_df = spark.read.csv(race_codes_path, header=True, inferSchema=True)\n",
    "\n",
    "# Join the crime data with the race codes to get the full description of 'Vict Descent'\n",
    "top_crime_with_race_df = top_crime_df.join(race_codes_df, top_crime_df[\"Vict Descent\"] == race_codes_df[\"Vict Descent\"], \"left_outer\")\n",
    "\n",
    "# Aggregate by 'Vict Descent Full' and count victims for top communities\n",
    "top_crime_race_count = top_crime_with_race_df.groupBy(\"Vict Descent Full\").agg(\n",
    "    F.count(\"*\").alias(\"Victim_Count\")\n",
    ").orderBy(F.col(\"Victim_Count\"), ascending=False)\n",
    "\n",
    "# Join the crime data with the race codes for bottom communities\n",
    "bottom_crime_with_race_df = bottom_crime_df.join(race_codes_df, bottom_crime_df[\"Vict Descent\"] == race_codes_df[\"Vict Descent\"], \"left_outer\")\n",
    "\n",
    "# Aggregate by 'Vict Descent Full' and count victims for bottom communities\n",
    "bottom_crime_race_count = bottom_crime_with_race_df.groupBy(\"Vict Descent Full\").agg(\n",
    "    F.count(\"*\").alias(\"Victim_Count\")\n",
    ").orderBy(F.col(\"Victim_Count\"), ascending=False)\n",
    "\n",
    "# Clean up NULL values by adding them to the existing 'Unknown' category\n",
    "top_crime_race_count_cleaned = top_crime_race_count.groupBy(\n",
    "    F.when(F.col(\"Vict Descent Full\").isNull(), \"Unknown\")\n",
    "    .otherwise(F.col(\"Vict Descent Full\")).alias(\"Vict Descent Full\")\n",
    ").agg(\n",
    "    F.sum(\"Victim_Count\").alias(\"Victim_Count\")\n",
    ").orderBy(F.col(\"Victim_Count\"), ascending=False)\n",
    "\n",
    "bottom_crime_race_count_cleaned = bottom_crime_race_count.groupBy(\n",
    "    F.when(F.col(\"Vict Descent Full\").isNull(), \"Unknown\")\n",
    "    .otherwise(F.col(\"Vict Descent Full\")).alias(\"Vict Descent Full\")\n",
    ").agg(\n",
    "    F.sum(\"Victim_Count\").alias(\"Victim_Count\")\n",
    ").orderBy(F.col(\"Victim_Count\"), ascending=False)\n",
    "\n",
    "\n",
    "# You can now show the cleaned-up DataFrames or export them\n",
    "top_crime_race_count_cleaned.show(truncate=False)\n",
    "bottom_crime_race_count_cleaned.show(truncate=False)\n",
    "\n",
    "# Stop timing and print out the execution duration\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b9ba296-280f-438e-842c-4230671ee104",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "invalid syntax (<stdin>, line 1)\n",
      "  File \"<stdin>\", line 1\n",
      "    The observed times for the three different configurations suggest that, as the resources allocated to Spark increase (in terms of CPU cores and memory), the performance of the analysis improves, but the improvement is not always drastic. Let's break down and analyze the results:\n",
      "        ^\n",
      "SyntaxError: invalid syntax\n",
      "\n"
     ]
    }
   ],
   "source": [
    "The observed times for the three different configurations suggest that, as the resources allocated to Spark increase (in terms of CPU cores and memory), the performance of the analysis improves, but the improvement is not always drastic. Let's break down and analyze the results:\n",
    "\n",
    "### Results Overview:\n",
    "1. **First Case (1 core / 2 GB memory per executor)**: \n",
    "   - **Time Taken**: 101.66 seconds.\n",
    "   \n",
    "2. **Second Case (2 cores / 4 GB memory per executor)**: \n",
    "   - **Time Taken**: 101.32 seconds.\n",
    "\n",
    "3. **Third Case (4 cores / 8 GB memory per executor)**: \n",
    "   - **Time Taken**: 84.34 seconds.\n",
    "\n",
    "### Observations:\n",
    "- **First and Second Cases**: \n",
    "  - The time taken between the first and second configurations is almost identical (101.66 seconds vs 101.32 seconds). This suggests that doubling the number of cores and memory had little to no effect on the performance for this workload. It's possible that the task you are running is not highly parallelizable or that it is limited by other factors (such as disk I/O, network latency, or the size of data not being large enough to fully utilize the extra cores and memory).\n",
    "  \n",
    "- **Second and Third Cases**: \n",
    "  - The third case (4 cores and 8 GB memory) shows a noticeable improvement (84.34 seconds) compared to the second case (101.32 seconds). This suggests that with more cores and memory, Spark can execute more operations in parallel or handle larger intermediate datasets more efficiently. The improvement here indicates that the workload benefits from additional parallelism, but the returns diminish with each increase in resources.\n",
    "  \n",
    "### Possible Reasons for These Observations:\n",
    "1. **Diminishing Returns**: As more cores and memory are allocated, the performance improvements tend to diminish. This is typical for many types of data processing workloads. For small to moderate-sized tasks, you might not see a large speedup when doubling the resources because the overhead of managing these resources (e.g., task scheduling, memory management) may offset some of the potential benefits.\n",
    "  \n",
    "2. **Task Characteristics**: \n",
    "   - The nature of the task matters. For tasks that are highly parallelizable and require significant computation (e.g., large-scale matrix operations or iterative algorithms), adding more cores and memory could lead to substantial performance gains. However, for tasks that are more dependent on I/O or where the data size is small enough to fit comfortably in the available memory, adding more resources might not show as dramatic an improvement.\n",
    "  \n",
    "3. **Data Size**: \n",
    "   - If your dataset is not large enough, Spark might not fully utilize the additional cores. In such cases, the time for job execution may be limited by other factors such as network bandwidth, disk read/write speed, or the execution overhead.\n",
    "  \n",
    "4. **Job Complexity**: \n",
    "   - The job you're running may not have enough computationally intensive stages to benefit from additional cores. For example, if your job spends time waiting for I/O operations or is CPU-bound on a relatively small amount of data, adding more executors might not significantly speed up execution.\n",
    "\n",
    "### Conclusion:\n",
    "While the third configuration (4 cores / 8 GB memory) shows the best performance, the results indicate that for your particular workload, the benefits of increasing resources (cores and memory) are relatively modest. In particular, going from 1 core to 2 cores did not show significant improvement, and the performance improvement from 2 cores to 4 cores is noticeable but not drastic.\n",
    "\n",
    "For further analysis:\n",
    "- You could experiment with larger datasets or more complex operations to see if the resource allocation truly impacts performance.\n",
    "- Monitoring the resource utilization during the run (e.g., CPU and memory usage) can provide insights into whether resources are being fully utilized.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

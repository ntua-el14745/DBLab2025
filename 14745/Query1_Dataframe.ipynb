{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8bfd658-378c-4a40-8186-296a935c14db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 7.61 seconds\n",
      "+------------+------+\n",
      "|   Age_Group| count|\n",
      "+------------+------+\n",
      "|      Adults|121052|\n",
      "|Young Adults| 33588|\n",
      "|    Children| 15923|\n",
      "|     Seniors|  5985|\n",
      "+------------+------+"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import trim, lower, col, when, desc\n",
    "import time\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query1_DataFrame\") \\\n",
    "    .config(\"spark.executor.instances\", 4) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "# Load crime data\n",
    "# Load the 2010-2019 crime data\n",
    "crime_df_2010_2019 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", \n",
    "    header=True\n",
    ")\n",
    "\n",
    "# Load the 2020-present crime data\n",
    "crime_df_2020_present = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\", \n",
    "    header=True\n",
    ")\n",
    "crime_df = crime_df_2010_2019.union(crime_df_2020_present)\n",
    "\n",
    "\n",
    "# crime_df.select(\"Crm Cd Desc\", \"LAT\", \"LON\").show(10, truncate=False)\n",
    "\n",
    "# Clean up 'Crm Cd Desc' column and apply filters\n",
    "filtered_df = crime_df.filter(\n",
    "    (lower(trim(col(\"Crm Cd Desc\"))).contains(\"aggravated assault\")) &  # Case-insensitive match\n",
    "    (col(\"LAT\").isNotNull() & col(\"LON\").isNotNull() & (col(\"LAT\") != 0) & (col(\"LON\") != 0))  # Valid coordinates\n",
    ")\n",
    "\n",
    "# filtered_df.select(\"Crm Cd Desc\", \"LAT\", \"LON\").show(10, truncate=False)\n",
    "# filtered_df.count()\n",
    "\n",
    "# Add Age Group column based on 'Vict Age'\n",
    "age_grouped_df = filtered_df.withColumn(\n",
    "    \"Age_Group\",\n",
    "    when(col(\"Vict Age\").cast(\"int\") < 18, \"Children\")\n",
    "    .when((col(\"Vict Age\").cast(\"int\") >= 18) & (col(\"Vict Age\").cast(\"int\") <= 24), \"Young Adults\")\n",
    "    .when((col(\"Vict Age\").cast(\"int\") >= 25) & (col(\"Vict Age\").cast(\"int\") <= 64), \"Adults\")\n",
    "    .when(col(\"Vict Age\").cast(\"int\") > 64, \"Seniors\")\n",
    ")\n",
    "\n",
    "# Group by Age Group and count incidents, then sort\n",
    "result_df = age_grouped_df.groupBy(\"Age_Group\").count().orderBy(desc(\"count\"))\n",
    "\n",
    "# Stop timing and print out the execution duration\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Write results\n",
    "group_number = \"24\"\n",
    "\n",
    "s3_path = \"s3://groups-bucket-dblab-905418150721/group\"+group_number+\"/results/\"\n",
    "\n",
    "result_df.write.mode(\"overwrite\").parquet(s3_path + \"q1_dataframe_output\")\n",
    "\n",
    "# Show results\n",
    "result_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a403232-5242-43b8-b830-c091c01b5fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarized\n",
    "\n",
    "# The performance difference between DataFrame and RDD versions in Spark is mainly due to several factors:\n",
    "\n",
    "# 1. **Optimized Execution (DataFrame API)**: DataFrames benefit from the **Catalyst Optimizer**, which automatically applies optimizations like predicate pushdown and logical plan optimization. These optimizations reduce the amount of data processed and increase efficiency.\n",
    "\n",
    "# 2. **Manual Optimizations (RDD)**: RDDs lack these optimizations, resulting in less efficient execution. RDD operations are interpreted at runtime, without Spark’s ability to generate optimized bytecode or automatically apply filters early in the process.\n",
    "\n",
    "# 3. **Efficient Data Representation**: DataFrames use a columnar storage format (Tungsten engine), which is more efficient for filtering and aggregation. RDDs use a row-based structure, which is less efficient for similar operations.\n",
    "\n",
    "# 4. **Parallelism and Distribution**: DataFrames manage parallelism and distribution more effectively than RDDs. Spark optimizes DataFrame queries for better partitioning and task scheduling, reducing overhead.\n",
    "\n",
    "# 5. **Cluster Overhead**: RDDs may incur more overhead due to manual partitioning and more complex operations, while DataFrames are optimized for distributed execution, reducing unnecessary shuffling and communication.\n",
    "\n",
    "# In summary, **DataFrames** are generally faster than **RDDs** for structured data processing due to built-in optimizations, making them the preferred choice for most Spark operations.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# The difference in execution times between the DataFrame and RDD versions in Spark can be attributed to several factors related to the underlying design and optimizations of each approach. Here's an explanation of why you might see the discrepancy:\n",
    "\n",
    "# ### 1. **Optimized Execution (DataFrame API)**:\n",
    "#    - **Catalyst Optimizer**: The DataFrame API in Spark leverages the **Catalyst Optimizer**, which is an advanced query optimizer. The optimizer automatically applies several optimizations, such as:\n",
    "#      - Predicate pushdown: Conditions like `age < 18` and filtering on `Crm Cd Desc` are applied as early as possible in the execution plan, minimizing the amount of data processed.\n",
    "#      - Logical plan optimization: The Catalyst optimizer transforms logical plans into physical plans that are optimized for better performance.\n",
    "#      - Whole-stage code generation: This allows Spark to generate Java bytecode that is more efficient than interpreting each operation in the RDD-based approach.\n",
    "#    - **Built-in Optimizations**: DataFrames abstract away a lot of the complexity and apply optimizations like filtering, projection, and partitioning automatically. This makes DataFrame operations faster.\n",
    "\n",
    "# ### 2. **RDD Operations (Low-Level API)**:\n",
    "#    - **Manual Optimizations**: With RDDs, Spark doesn’t apply the same level of optimization as with DataFrames. Operations on RDDs are executed in a more manual and unoptimized way, meaning that Spark can't perform some optimizations like predicate pushdown or plan optimization.\n",
    "#    - **Serialization and Deserialization**: When using RDDs, Spark may need to serialize and deserialize the data more frequently, which can add overhead, especially when passing data between stages in a distributed environment.\n",
    "#    - **Lack of Whole-Stage Code Generation**: RDD transformations don't benefit from Spark's whole-stage code generation, which is a significant optimization available with DataFrames and Datasets. This makes RDD transformations slower because Spark has to interpret each operation at runtime instead of executing pre-compiled bytecode.\n",
    "\n",
    "# ### 3. **Data Representation**:\n",
    "#    - **DataFrames vs. RDDs**: DataFrames use a more optimized, columnar format (Tungsten execution engine), whereas RDDs use a row-based structure. Columnar storage allows for more efficient processing and memory utilization, particularly for queries involving filtering and aggregations.\n",
    "#    - **Schema Handling**: DataFrames come with schema information (column names, types), which allows Spark to optimize the operations. With RDDs, you have to handle the data in a more unstructured way (e.g., using dictionaries or tuples), which can be less efficient.\n",
    "\n",
    "# ### 4. **Parallelism and Distribution**:\n",
    "#    - **RDDs**: When working with RDDs, Spark requires more manual control over how data is distributed and processed. While this provides flexibility, it can lead to less efficient parallelism and task scheduling compared to DataFrames, which internally take care of the distribution and parallel execution.\n",
    "#    - **DataFrames**: Spark can execute DataFrame queries in parallel more efficiently by using optimizations like partitioning, broadcasting, and better scheduling.\n",
    "\n",
    "# ### 5. **Code Complexity**:\n",
    "#    - The RDD approach typically involves more explicit loops and operations, which can introduce additional computational overhead. On the other hand, DataFrames abstract away much of the complexity and handle multiple operations in a single execution plan, leading to better performance.\n",
    "\n",
    "# ### 6. **Cluster Overhead**:\n",
    "#    - **RDD**: RDDs may also face higher overhead when performing operations across partitions. Since RDD operations are lower-level and do not benefit from some of the optimizations in the DataFrame API, they can result in more shuffling and network overhead.\n",
    "#    - **DataFrames**: DataFrame operations are more aware of partitioning and distributed execution, which means they generally reduce unnecessary shuffling and minimize the communication overhead between nodes.\n",
    "\n",
    "# ### 7. **Spark Version and Configuration**:\n",
    "#    - The Spark version and configurations used can also impact the performance difference. DataFrames are generally faster, but if you have specific configurations that make RDD operations more efficient (like optimized serialization or caching), that could mitigate the gap.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### Summary:\n",
    "# - **RDDs** are more flexible but require you to manually handle optimizations, leading to higher overhead for the same operations.\n",
    "# - **DataFrames** benefit from **Catalyst Optimizer**, **whole-stage code generation**, and **automatic optimizations**, which result in much faster execution times for queries like yours.\n",
    "\n",
    "# Given that the DataFrame version outperforms the RDD version in this case, it's generally recommended to use DataFrames for most Spark operations, especially when working with structured data and performing complex filtering and transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c346cdc2-6279-4a37-a8b9-e917d4cb2326",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

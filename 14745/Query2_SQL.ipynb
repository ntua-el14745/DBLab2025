{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b742113-d077-4ffe-a489-c114e86cb59f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+------------------+----+\n",
      "|Year|Area       |Closed_Case_Rate  |Rank|\n",
      "+----+-----------+------------------+----+\n",
      "|2010|Rampart    |32.84713448949121 |1   |\n",
      "|2010|Olympic    |31.515289821999087|2   |\n",
      "|2010|Harbor     |29.36028339237341 |3   |\n",
      "|2011|Olympic    |35.040060090135206|1   |\n",
      "|2011|Rampart    |32.4964471814306  |2   |\n",
      "|2011|Harbor     |28.51336246316431 |3   |\n",
      "|2012|Olympic    |34.29708533302119 |1   |\n",
      "|2012|Rampart    |32.46000463714352 |2   |\n",
      "|2012|Harbor     |29.509585848956675|3   |\n",
      "|2013|Olympic    |33.58217940999398 |1   |\n",
      "|2013|Rampart    |32.1060382916053  |2   |\n",
      "|2013|Harbor     |29.723638951488557|3   |\n",
      "|2014|Van Nuys   |32.0215235281705  |1   |\n",
      "|2014|West Valley|31.49754809505847 |2   |\n",
      "|2014|Mission    |31.224939855653567|3   |\n",
      "|2015|Van Nuys   |32.265140677157845|1   |\n",
      "|2015|Mission    |30.463762673676303|2   |\n",
      "|2015|Foothill   |30.353001803658852|3   |\n",
      "|2016|Van Nuys   |32.194518462124094|1   |\n",
      "|2016|West Valley|31.40146437042384 |2   |\n",
      "+----+-----------+------------------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----+-----------+------------------+----+\n",
      "|Year|Area       |Closed_Case_Rate  |Rank|\n",
      "+----+-----------+------------------+----+\n",
      "|2010|Rampart    |32.84713448949121 |1   |\n",
      "|2010|Olympic    |31.515289821999087|2   |\n",
      "|2010|Harbor     |29.36028339237341 |3   |\n",
      "|2011|Olympic    |35.040060090135206|1   |\n",
      "|2011|Rampart    |32.4964471814306  |2   |\n",
      "|2011|Harbor     |28.51336246316431 |3   |\n",
      "|2012|Olympic    |34.29708533302119 |1   |\n",
      "|2012|Rampart    |32.46000463714352 |2   |\n",
      "|2012|Harbor     |29.509585848956675|3   |\n",
      "|2013|Olympic    |33.58217940999398 |1   |\n",
      "|2013|Rampart    |32.1060382916053  |2   |\n",
      "|2013|Harbor     |29.723638951488557|3   |\n",
      "|2014|Van Nuys   |32.0215235281705  |1   |\n",
      "|2014|West Valley|31.49754809505847 |2   |\n",
      "|2014|Mission    |31.224939855653567|3   |\n",
      "|2015|Van Nuys   |32.265140677157845|1   |\n",
      "|2015|Mission    |30.463762673676303|2   |\n",
      "|2015|Foothill   |30.353001803658852|3   |\n",
      "|2016|Van Nuys   |32.194518462124094|1   |\n",
      "|2016|West Valley|31.40146437042384 |2   |\n",
      "+----+-----------+------------------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken: 30.02 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import time\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Query2_SQL\") \\\n",
    "    .config(\"spark.executor.instances\", 4) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Load the 2010-2019 crime data\n",
    "crime_df_2010_2019 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", \n",
    "    header=True\n",
    ")\n",
    "\n",
    "# Load the 2020-present crime data\n",
    "crime_df_2020_present = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\", \n",
    "    header=True\n",
    ")\n",
    "crime_df = crime_df_2010_2019.union(crime_df_2020_present)\n",
    "\n",
    "# Clean up the data and classify cases as 'Open' or 'Closed'\n",
    "crime_df = crime_df.withColumn(\n",
    "    \"Case_Status\",\n",
    "    F.when(F.col(\"Status Desc\").isin(\"UNK\", \"Invest Cont\"), \"Open\").otherwise(\"Closed\")\n",
    ")\n",
    "\n",
    "# Create a new column 'Year' based on the 'Date Rptd' timestamp column\n",
    "crime_df = crime_df.withColumn(\"Year\", F.year(F.to_timestamp(\"DATE OCC\", \"MM/dd/yyyy hh:mm:ss a\")))\n",
    "\n",
    "# Register the DataFrame as a temporary SQL view\n",
    "crime_df.createOrReplaceTempView(\"crime_data\")\n",
    "\n",
    "# SQL query to find the top 3 precincts with the highest closed case rates per year\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    Year,\n",
    "    Area,\n",
    "    Closed_Case_Rate,\n",
    "    Rank\n",
    "FROM (\n",
    "    SELECT \n",
    "        Year,\n",
    "        `AREA NAME` AS Area,\n",
    "        (SUM(CASE WHEN Case_Status = 'Closed' THEN 1 ELSE 0 END) / COUNT(*) * 100) AS Closed_Case_Rate,\n",
    "        ROW_NUMBER() OVER (PARTITION BY Year ORDER BY (SUM(CASE WHEN Case_Status = 'Closed' THEN 1 ELSE 0 END) / COUNT(*) * 100) DESC) AS Rank\n",
    "    FROM \n",
    "        crime_data\n",
    "    GROUP BY \n",
    "        Year, `AREA NAME`\n",
    ") AS subquery\n",
    "WHERE Rank <= 3\n",
    "ORDER BY Year, Rank\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "result_df = spark.sql(query)\n",
    "\n",
    "# Show the results\n",
    "result_df.show(truncate=False)\n",
    "\n",
    "# Stop timing and print out the execution duration\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Write results to S3 in Parquet format\n",
    "group_number = \"24\"\n",
    "s3_path = \"s3://groups-bucket-dblab-905418150721/group\"+group_number+\"/results/\"\n",
    "\n",
    "result_df.write.mode(\"overwrite\").parquet(s3_path + \"q2_sql_output\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bee830-9b2c-4207-9d06-e7d084e7e739",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8df66e2d-359d-4793-82f6-bf04283d8b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from sedona.register.geo_registrator import SedonaRegistrator\n",
    "from sedona.spark import SedonaContext\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Initialize Spark Session and Sedona Context\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CrimeAnalysisSpatialJoinSQL\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.sedona.sql.SedonaSqlExtensions\") \\\n",
    "    .config(\"sedona.global.charset\", \"utf8\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sedona = SedonaContext.create(spark)\n",
    "\n",
    "# Load datasets\n",
    "census_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "income_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\"\n",
    "crime_path = \"s3://groups-bucket-dblab-905418150721/group24/results/q2_parquet_maindata/\"\n",
    "\n",
    "# Load Census Data (GeoJSON format)\n",
    "census_df = sedona.read.format(\"geojson\") \\\n",
    "    .option(\"multiLine\", \"true\").load(census_path) \\\n",
    "    .selectExpr(\"explode(features) as features\") \\\n",
    "    .select(\"features.*\")\n",
    "\n",
    "# Flatten the GeoJSON structure and filter valid populations\n",
    "census_df = census_df.select(\n",
    "    F.col(\"properties.ZCTA10\").alias(\"ZCTA10\"),\n",
    "    F.col(\"properties.POP_2010\").alias(\"Population\"),\n",
    "    F.col(\"properties.COMM\").alias(\"COMM\"),\n",
    "    F.col(\"geometry\").alias(\"geometry\"),\n",
    "    F.col(\"properties.HOUSING10\").alias(\"HOUSING10\"),\n",
    ").filter(F.col(\"Population\") > 0)  # Exclude zero or negative population\n",
    "\n",
    "# Load Income Data (CSV format)\n",
    "income_df = spark.read.csv(income_path, header=True, inferSchema=True)\n",
    "income_df = income_df.withColumn(\n",
    "    \"Income\",\n",
    "    F.regexp_replace(F.col(\"Estimated Median Income\"), \"[$,]\", \"\").cast(\"double\")\n",
    ").withColumnRenamed(\"Zip Code\",\"Zip\").drop(\"Estimated Median Income\")\n",
    "\n",
    "# Calculate total income per community (sum of income contributions per zip)\n",
    "income_total = census_df.join(income_df, census_df.ZCTA10 == income_df.Zip, \"inner\") \\\n",
    "    .groupBy(\"COMM\").agg(\n",
    "        F.sum(F.col(\"Income\") * F.col(\"HOUSING10\")).alias(\"Total_Income\")\n",
    "    )\n",
    "\n",
    "# Aggregate census data by community\n",
    "census_agg = census_df.groupBy(\"COMM\").agg(\n",
    "    F.sum(\"Population\").alias(\"Total_Population\"),\n",
    "    F.sum(\"HOUSING10\").alias(\"Total_Households\")\n",
    ")\n",
    "\n",
    "# Join census and income data\n",
    "census_income = census_agg.join(income_total, \"COMM\", \"inner\")\n",
    "\n",
    "# Calculate Mean Income Per Person\n",
    "census_income = census_income.withColumn(\n",
    "    \"Mean_Income_Per_Person\",\n",
    "    F.col(\"Total_Income\") / F.col(\"Total_Population\")\n",
    ")\n",
    "\n",
    "# Step 1: Get the top 3 and bottom 3 communities by Mean Income Per Person\n",
    "top_communities = census_income.orderBy(F.col(\"Mean_Income_Per_Person\"), ascending=False).limit(3)\n",
    "bottom_communities = census_income.orderBy(F.col(\"Mean_Income_Per_Person\"), ascending=True).limit(3)\n",
    "\n",
    "# Step 2: Filter crime data for top and bottom communities\n",
    "top_community_codes = [row['COMM'] for row in top_communities.collect()]\n",
    "bottom_community_codes = [row['COMM'] for row in bottom_communities.collect()]\n",
    "\n",
    "# Load Crime Data (Parquet format)\n",
    "crime_df = spark.read.parquet(crime_path)\n",
    "\n",
    "# Create geometry column using ST_Point\n",
    "crime_df = crime_df.withColumn(\"geometry\", F.expr(\"ST_Point(LON, LAT)\"))\n",
    "\n",
    "# Step 3: Join the crime data with census data to include 'COMM' (Community) info\n",
    "crime_with_comm_df = crime_df.alias(\"cr\").join(\n",
    "    census_df.alias(\"c\"),\n",
    "    F.expr(\"ST_Within(cr.geometry, c.geometry)\"),\n",
    "    \"inner\"\n",
    ")\n",
    "# .select(\n",
    "#     \"census_df.COMM\"\n",
    "# )\n",
    "\n",
    "# Filter crime data for top communities (those with highest income)\n",
    "top_crime_df = crime_with_comm_df.filter(F.col(\"COMM\").isin(top_community_codes))\n",
    "\n",
    "# Filter crime data for bottom communities (those with lowest income)\n",
    "bottom_crime_df = crime_with_comm_df.filter(F.col(\"COMM\").isin(bottom_community_codes))\n",
    "\n",
    "# Step 4: Load race codes dataset to translate 'Vict Descent'\n",
    "race_codes_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/RE_codes.csv\"\n",
    "race_codes_df = spark.read.csv(race_codes_path, header=True, inferSchema=True)\n",
    "\n",
    "# Join the crime data with the race codes to get the full description of 'Vict Descent'\n",
    "top_crime_with_race_df = top_crime_df.join(race_codes_df, top_crime_df[\"Vict Descent\"] == race_codes_df[\"Vict Descent\"], \"left_outer\")\n",
    "\n",
    "# Aggregate by 'Vict Descent Full' and count victims for top communities\n",
    "top_crime_race_count = top_crime_with_race_df.groupBy(\"Vict Descent Full\").agg(\n",
    "    F.count(\"*\").alias(\"Victim_Count\")\n",
    ").orderBy(F.col(\"Victim_Count\"), ascending=False)\n",
    "\n",
    "# Join the crime data with the race codes for bottom communities\n",
    "bottom_crime_with_race_df = bottom_crime_df.join(race_codes_df, bottom_crime_df[\"Vict Descent\"] == race_codes_df[\"Vict Descent\"], \"left_outer\")\n",
    "\n",
    "# Aggregate by 'Vict Descent Full' and count victims for bottom communities\n",
    "bottom_crime_race_count = bottom_crime_with_race_df.groupBy(\"Vict Descent Full\").agg(\n",
    "    F.count(\"*\").alias(\"Victim_Count\")\n",
    ").orderBy(F.col(\"Victim_Count\"), ascending=False)\n",
    "\n",
    "# Step 5: Export the results to new files\n",
    "# top_crime_race_count.write.option(\"header\", \"true\", \"overwrite\").csv(\"top_communities_race_breakdown.csv\")\n",
    "# bottom_crime_race_count.write.option(\"header\", \"true\", \"overwrite\").csv(\"bottom_communities_race_breakdown.csv\")\n",
    "\n",
    "# print(\"Top and Bottom Communities Crime Race Breakdown exported to CSV files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66ce0229-3009-4b82-824d-54d2cc4b0c47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|   Vict Descent Full|Victim_Count|\n",
      "+--------------------+------------+\n",
      "|               White|        5726|\n",
      "|               Other|         690|\n",
      "|Hispanic/Latin/Me...|         558|\n",
      "|                NULL|         477|\n",
      "|             Unknown|         359|\n",
      "|               Black|         260|\n",
      "|         Other Asian|         204|\n",
      "|             Chinese|          29|\n",
      "|            Japanese|          20|\n",
      "|            Filipino|          16|\n",
      "|              Korean|          13|\n",
      "|         AsianIndian|           5|\n",
      "|American Indian/A...|           4|\n",
      "|          Vietnamese|           3|\n",
      "|            Hawaiian|           1|\n",
      "+--------------------+------------+\n",
      "\n",
      "+--------------------+------------+\n",
      "|   Vict Descent Full|Victim_Count|\n",
      "+--------------------+------------+\n",
      "|Hispanic/Latin/Me...|       19703|\n",
      "|               Black|        6316|\n",
      "|                NULL|        4715|\n",
      "|               White|        1469|\n",
      "|               Other|         807|\n",
      "|             Unknown|         362|\n",
      "|         Other Asian|         166|\n",
      "|            Filipino|          12|\n",
      "|            Japanese|           8|\n",
      "|              Korean|           8|\n",
      "|             Laotian|           5|\n",
      "|             Chinese|           4|\n",
      "|American Indian/A...|           4|\n",
      "|          Vietnamese|           3|\n",
      "|           Cambodian|           3|\n",
      "|    Pacific Islander|           3|\n",
      "|           Guamanian|           2|\n",
      "|            Hawaiian|           1|\n",
      "+--------------------+------------+"
     ]
    }
   ],
   "source": [
    "top_crime_race_count.show()\n",
    "bottom_crime_race_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b80e5a-f79c-4353-9249-3c5f412f0c17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

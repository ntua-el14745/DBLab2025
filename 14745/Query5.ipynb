{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d12d24f8-4909-44f5-9d81-9cb3a9ac4cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1943</td><td>application_1732639283265_1904</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1904/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-193.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1904_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+------------------+\n",
      "|        DIVISION|crime_count|  average_distance|\n",
      "+----------------+-----------+------------------+\n",
      "|       HOLLYWOOD|     224340| 2.076263960178773|\n",
      "|        VAN NUYS|     210134|2.9533697428196213|\n",
      "|       SOUTHWEST|     188901|2.1913988057806097|\n",
      "|        WILSHIRE|     185996|2.5926655329788635|\n",
      "|     77TH STREET|     171827|1.7165449719700194|\n",
      "|         OLYMPIC|     170897| 1.723603697177835|\n",
      "| NORTH HOLLYWOOD|     167854| 2.643006094141679|\n",
      "|         PACIFIC|     161359|  3.85007065530769|\n",
      "|         CENTRAL|     153871| 0.992476437456772|\n",
      "|         RAMPART|     152736|1.5345341879189556|\n",
      "|       SOUTHEAST|     152176|2.4218662158882407|\n",
      "|     WEST VALLEY|     138643|3.0356712163141375|\n",
      "|         TOPANGA|     138217| 3.296954841755896|\n",
      "|        FOOTHILL|     134896| 4.250921708425352|\n",
      "|          HARBOR|     126747|3.7025615993562746|\n",
      "|      HOLLENBECK|     119294|366.92130446149946|\n",
      "|WEST LOS ANGELES|     115781| 2.792457289033499|\n",
      "|          NEWTON|     111110|1.6346357397096194|\n",
      "|       NORTHEAST|     108109|3.6236655246041707|\n",
      "|         MISSION|     103355|3.6909426142787916|\n",
      "+----------------+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken: 107.03 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "\n",
    "# Initialize Spark Session and Sedona Context\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CrimeAnalysisQuery5\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.sedona.sql.SedonaSqlExtensions\") \\\n",
    "    .config(\"sedona.global.charset\", \"utf8\") \\\n",
    "    .config(\"spark.executor.instances\", 2) \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.cores\", 4) \\\n",
    "    .getOrCreate()\n",
    "    # .config(\"spark.driver.memory\", \"4g\") \\\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "# Load Police Stations Data (CSV format)\n",
    "police_station_data_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/LA_Police_Stations.csv\"\n",
    "police_df = spark.read.csv(police_station_data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Create a geometry column for police stations using ST_Point\n",
    "police_df = police_df.withColumn(\"station_point\", F.expr(\"ST_Point(X, Y)\"))\n",
    "\n",
    "crime_data_path = \"s3://groups-bucket-dblab-905418150721/group24/results/q2_parquet_maindata/\"\n",
    "crime_df = spark.read.parquet(crime_data_path)\n",
    "\n",
    "# Create geometry point from latitude and longitude\n",
    "crime_df = crime_df.withColumn(\"crime_point\", F.expr(\"ST_Point(LON, LAT)\"))\n",
    "\n",
    "\n",
    "# Add unique ID to each crime record\n",
    "crime_df = crime_df.withColumn(\"crime_id\", F.row_number().over(Window.orderBy(F.lit(1))))\n",
    "\n",
    "# Cross join crime and police station data\n",
    "cross_df = crime_df.crossJoin(police_df)\n",
    "\n",
    "# Calculate distance\n",
    "cross_df = cross_df.withColumn(\"distance\", F.expr(\"ST_DistanceSphere(crime_point, station_point)/1000\"))\n",
    "\n",
    "# Find the nearest police station for each crime\n",
    "window_spec = Window.partitionBy(\"crime_id\").orderBy(\"distance\")\n",
    "nearest_station_df = cross_df.withColumn(\"rank\", F.row_number().over(window_spec)).filter(F.col(\"rank\") == 1)\n",
    "\n",
    "results_df = nearest_station_df.groupBy(\"DIVISION\").agg(\n",
    "    F.count(\"*\").alias(\"crime_count\"),\n",
    "    F.avg(\"distance\").alias(\"average_distance\")\n",
    ").orderBy(F.col(\"crime_count\").desc())\n",
    "\n",
    "results_df.show()\n",
    "\n",
    "# Stop timing and print out the execution duration\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88d25459-75fa-49d2-8ea7-1fcd6349e112",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+------------------+\n",
      "|        DIVISION|crime_count|  average_distance|\n",
      "+----------------+-----------+------------------+\n",
      "|       HOLLYWOOD|     224340| 2.076263960178773|\n",
      "|        VAN NUYS|     210134|2.9533697428196213|\n",
      "|       SOUTHWEST|     188901|2.1913988057806097|\n",
      "|        WILSHIRE|     185996|2.5926655329788635|\n",
      "|     77TH STREET|     171827|1.7165449719700194|\n",
      "|         OLYMPIC|     170897| 1.723603697177835|\n",
      "| NORTH HOLLYWOOD|     167854| 2.643006094141679|\n",
      "|         PACIFIC|     161359|  3.85007065530769|\n",
      "|         CENTRAL|     153871| 0.992476437456772|\n",
      "|         RAMPART|     152736|1.5345341879189556|\n",
      "|       SOUTHEAST|     152176|2.4218662158882407|\n",
      "|     WEST VALLEY|     138643|3.0356712163141375|\n",
      "|         TOPANGA|     138217| 3.296954841755896|\n",
      "|        FOOTHILL|     134896| 4.250921708425352|\n",
      "|          HARBOR|     126747|3.7025615993562746|\n",
      "|      HOLLENBECK|     119294|366.92130446149946|\n",
      "|WEST LOS ANGELES|     115781| 2.792457289033499|\n",
      "|          NEWTON|     111110|1.6346357397096194|\n",
      "|       NORTHEAST|     108109|3.6236655246041707|\n",
      "|         MISSION|     103355|3.6909426142787916|\n",
      "+----------------+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken: 84.12 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "\n",
    "# Initialize Spark Session and Sedona Context\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CrimeAnalysisQuery5\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.sedona.sql.SedonaSqlExtensions\") \\\n",
    "    .config(\"sedona.global.charset\", \"utf8\") \\\n",
    "    .config(\"spark.executor.instances\", 4) \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", 2) \\\n",
    "    .getOrCreate()\n",
    "    # .config(\"spark.driver.memory\", \"4g\") \\\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "# Load Police Stations Data (CSV format)\n",
    "police_station_data_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/LA_Police_Stations.csv\"\n",
    "police_df = spark.read.csv(police_station_data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Create a geometry column for police stations using ST_Point\n",
    "police_df = police_df.withColumn(\"station_point\", F.expr(\"ST_Point(X, Y)\"))\n",
    "\n",
    "crime_data_path = \"s3://groups-bucket-dblab-905418150721/group24/results/q2_parquet_maindata/\"\n",
    "crime_df = spark.read.parquet(crime_data_path)\n",
    "\n",
    "# Create geometry point from latitude and longitude\n",
    "crime_df = crime_df.withColumn(\"crime_point\", F.expr(\"ST_Point(LON, LAT)\"))\n",
    "\n",
    "\n",
    "# Add unique ID to each crime record\n",
    "crime_df = crime_df.withColumn(\"crime_id\", F.row_number().over(Window.orderBy(F.lit(1))))\n",
    "\n",
    "# Cross join crime and police station data\n",
    "cross_df = crime_df.crossJoin(police_df)\n",
    "\n",
    "# Calculate distance\n",
    "cross_df = cross_df.withColumn(\"distance\", F.expr(\"ST_DistanceSphere(crime_point, station_point)/1000\"))\n",
    "\n",
    "# Find the nearest police station for each crime\n",
    "window_spec = Window.partitionBy(\"crime_id\").orderBy(\"distance\")\n",
    "nearest_station_df = cross_df.withColumn(\"rank\", F.row_number().over(window_spec)).filter(F.col(\"rank\") == 1)\n",
    "\n",
    "results_df = nearest_station_df.groupBy(\"DIVISION\").agg(\n",
    "    F.count(\"*\").alias(\"crime_count\"),\n",
    "    F.avg(\"distance\").alias(\"average_distance\")\n",
    ").orderBy(F.col(\"crime_count\").desc())\n",
    "\n",
    "results_df.show()\n",
    "\n",
    "# Stop timing and print out the execution duration\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bd65f78-13a9-49aa-bfb3-3e94df853c19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+------------------+\n",
      "|        DIVISION|crime_count|  average_distance|\n",
      "+----------------+-----------+------------------+\n",
      "|       HOLLYWOOD|     224340| 2.076263960178773|\n",
      "|        VAN NUYS|     210134|2.9533697428196213|\n",
      "|       SOUTHWEST|     188901|2.1913988057806097|\n",
      "|        WILSHIRE|     185996|2.5926655329788635|\n",
      "|     77TH STREET|     171827|1.7165449719700194|\n",
      "|         OLYMPIC|     170897| 1.723603697177835|\n",
      "| NORTH HOLLYWOOD|     167854| 2.643006094141679|\n",
      "|         PACIFIC|     161359|  3.85007065530769|\n",
      "|         CENTRAL|     153871| 0.992476437456772|\n",
      "|         RAMPART|     152736|1.5345341879189556|\n",
      "|       SOUTHEAST|     152176|2.4218662158882407|\n",
      "|     WEST VALLEY|     138643|3.0356712163141375|\n",
      "|         TOPANGA|     138217| 3.296954841755896|\n",
      "|        FOOTHILL|     134896| 4.250921708425352|\n",
      "|          HARBOR|     126747|3.7025615993562746|\n",
      "|      HOLLENBECK|     119294|366.92130446149946|\n",
      "|WEST LOS ANGELES|     115781| 2.792457289033499|\n",
      "|          NEWTON|     111110|1.6346357397096194|\n",
      "|       NORTHEAST|     108109|3.6236655246041707|\n",
      "|         MISSION|     103355|3.6909426142787916|\n",
      "+----------------+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken: 82.28 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "\n",
    "# Initialize Spark Session and Sedona Context\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CrimeAnalysisQuery5\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.sedona.sql.SedonaSqlExtensions\") \\\n",
    "    .config(\"sedona.global.charset\", \"utf8\") \\\n",
    "    .config(\"spark.executor.instances\", 8) \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", 1) \\\n",
    "    .getOrCreate()\n",
    "    # .config(\"spark.driver.memory\", \"4g\") \\\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "# Load Police Stations Data (CSV format)\n",
    "police_station_data_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/LA_Police_Stations.csv\"\n",
    "police_df = spark.read.csv(police_station_data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Create a geometry column for police stations using ST_Point\n",
    "police_df = police_df.withColumn(\"station_point\", F.expr(\"ST_Point(X, Y)\"))\n",
    "\n",
    "crime_data_path = \"s3://groups-bucket-dblab-905418150721/group24/results/q2_parquet_maindata/\"\n",
    "crime_df = spark.read.parquet(crime_data_path)\n",
    "\n",
    "# Create geometry point from latitude and longitude\n",
    "crime_df = crime_df.withColumn(\"crime_point\", F.expr(\"ST_Point(LON, LAT)\"))\n",
    "\n",
    "\n",
    "# Add unique ID to each crime record\n",
    "crime_df = crime_df.withColumn(\"crime_id\", F.row_number().over(Window.orderBy(F.lit(1))))\n",
    "\n",
    "# Cross join crime and police station data\n",
    "cross_df = crime_df.crossJoin(police_df)\n",
    "\n",
    "# Calculate distance\n",
    "cross_df = cross_df.withColumn(\"distance\", F.expr(\"ST_DistanceSphere(crime_point, station_point)/1000\"))\n",
    "\n",
    "# Find the nearest police station for each crime\n",
    "window_spec = Window.partitionBy(\"crime_id\").orderBy(\"distance\")\n",
    "nearest_station_df = cross_df.withColumn(\"rank\", F.row_number().over(window_spec)).filter(F.col(\"rank\") == 1)\n",
    "\n",
    "results_df = nearest_station_df.groupBy(\"DIVISION\").agg(\n",
    "    F.count(\"*\").alias(\"crime_count\"),\n",
    "    F.avg(\"distance\").alias(\"average_distance\")\n",
    ").orderBy(F.col(\"crime_count\").desc())\n",
    "\n",
    "results_df.show()\n",
    "\n",
    "# Stop timing and print out the execution duration\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09806146-f5af-4f8d-b24a-d3e5c8ac4715",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The results indicate that increasing parallelism (more executors) slightly improves the runtime, but the differences are not substantial. Here's a breakdown of the observations:\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### **1. Interpretation of the Results**\n",
    "# - **2 executors × 4 cores/8GB memory:**\n",
    "#   - Total Resources: 8 cores, 16GB memory.\n",
    "#   - Time: 103.90 seconds.\n",
    "#   - Performance: This configuration had fewer executors but more resources per executor, resulting in fewer parallel tasks.\n",
    "\n",
    "# - **4 executors × 2 cores/4GB memory:**\n",
    "#   - Total Resources: 8 cores, 16GB memory.\n",
    "#   - Time: 101.33 seconds.\n",
    "#   - Performance: Increasing the number of executors reduced the workload per executor, leading to better utilization of parallelism.\n",
    "\n",
    "# - **8 executors × 1 core/2GB memory:**\n",
    "#   - Total Resources: 8 cores, 16GB memory.\n",
    "#   - Time: 99.94 seconds.\n",
    "#   - Performance: Further increasing the number of executors maximized parallelism and task distribution, resulting in the best time.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### **2. Key Insights**\n",
    "# 1. **Marginal Improvements:**  \n",
    "#    - The runtime improvements are marginal because the workload might not fully benefit from higher levels of parallelism. This suggests the task's bottleneck could be in I/O operations, data shuffling, or serialization/deserialization rather than computation.\n",
    "\n",
    "# 2. **Overhead of More Executors:**  \n",
    "#    - Adding more executors introduces some overhead in managing communication between them. However, the 8-executor configuration balanced this overhead with better task distribution, resulting in the fastest runtime.\n",
    "\n",
    "# 3. **Resource Allocation Trade-offs:**  \n",
    "#    - More cores per executor (2 executors × 4 cores) provide higher computational power per task, but it limits the number of tasks running in parallel.\n",
    "#    - Fewer cores per executor (8 executors × 1 core) increase the number of parallel tasks, which is beneficial if the workload can be split efficiently.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### **3. Recommendations**\n",
    "# - **Optimal Configuration:** Based on these results, the **8 executors × 1 core/2GB memory** setup is optimal for this query as it achieves the fastest time.\n",
    "# - **Further Tuning:**\n",
    "#   - If possible, reduce the data size being shuffled by optimizing the data partitioning strategy.\n",
    "#   - Experiment with Spark's default parallelism (`spark.default.parallelism`) or the number of shuffle partitions (`spark.sql.shuffle.partitions`) to further improve performance.\n",
    "# - **Monitor Resource Usage:** Use Spark's UI to monitor resource utilization (CPU, memory, disk, network). If utilization is low, further tuning may be necessary.\n",
    "\n",
    "# By balancing parallelism with resource allocation, you can achieve the best performance for workloads like this one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "730c9c0b-1484-4a22-b34d-f38be9587aba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+----------------+----------------------+------------+----------------------+\n",
      "|COMM                 |Total_Population|Mean_Income_Per_Person|Total_Crimes|Crime_Per_Person_Ratio|\n",
      "+---------------------+----------------+----------------------+------------+----------------------+\n",
      "|Culver City          |38883           |33644.97549057429     |345         |0.00887277216264177   |\n",
      "|North Lancaster      |1151            |19097.289313640315    |0           |0.0                   |\n",
      "|Rosewood/East Gardena|1164            |16165.823024054982    |101         |0.08676975945017182   |\n",
      "|East Rancho Dominguez|15135           |8830.036339610175     |0           |0.0                   |\n",
      "|Toluca Terrace       |1301            |20167.531898539586    |289         |0.22213681783243658   |\n",
      "|Elysian Park         |5267            |13865.32770077843     |3191        |0.6058477311562559    |\n",
      "|Longwood             |4210            |13420.052256532066    |3062        |0.7273159144893112    |\n",
      "|Pico Rivera          |62942           |15157.884512726001    |2           |3.177528518318452E-5  |\n",
      "|Malibu               |12645           |67046.98133649664     |1           |7.908264136022143E-5  |\n",
      "|Green Meadows        |19821           |8027.096412895414     |21961       |1.1079662983704153    |\n",
      "|Hacienda Heights     |53594           |24046.483673545546    |0           |0.0                   |\n",
      "|Cadillac-Corning     |6665            |19572.784696174043    |3877        |0.581695423855964     |\n",
      "|West Puente Valley   |9657            |12527.325566946256    |0           |0.0                   |\n",
      "|Montebello           |62500           |14514.115728          |6           |9.6E-5                |\n",
      "|Mid-city             |14339           |21734.64899923286     |10190       |0.7106492781923426    |\n",
      "|Lake Manor           |1600            |33720.028125          |0           |0.0                   |\n",
      "|Hawaiian Gardens     |14254           |9911.840535989897     |0           |0.0                   |\n",
      "|Lincoln Heights      |31144           |10767.81132802466     |15999       |0.5137105060364757    |\n",
      "|Westlake Village     |8270            |42843.458524788395    |1           |1.2091898428053205E-4 |\n",
      "|Van Nuys             |86019           |14488.189551145677    |67745       |0.787558562643137     |\n",
      "+---------------------+----------------+----------------------+------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Total communities: 319"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from sedona.register.geo_registrator import SedonaRegistrator\n",
    "from sedona.spark import SedonaContext\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Initialize Spark Session and Sedona Context\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CrimeAnalysisSpatialJoinSQL\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.sedona.sql.SedonaSqlExtensions\") \\\n",
    "    .config(\"sedona.global.charset\", \"utf8\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sedona = SedonaContext.create(spark)\n",
    "\n",
    "# Load datasets\n",
    "census_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "income_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\"\n",
    "crime_path = \"s3://groups-bucket-dblab-905418150721/group24/results/q2_parquet_maindata/\"\n",
    "\n",
    "# Load Census Data (GeoJSON format)\n",
    "census_df = sedona.read.format(\"geojson\") \\\n",
    "    .option(\"multiLine\", \"true\").load(census_path) \\\n",
    "    .selectExpr(\"explode(features) as features\") \\\n",
    "    .select(\"features.*\")\n",
    "\n",
    "# Flatten the GeoJSON structure and filter valid populations\n",
    "census_df = census_df.select(\n",
    "    F.col(\"properties.ZCTA10\").alias(\"ZCTA10\"),\n",
    "    F.col(\"properties.POP_2010\").alias(\"Population\"),\n",
    "    F.col(\"properties.COMM\").alias(\"COMM\"),\n",
    "    F.col(\"geometry\").alias(\"geometry\"),\n",
    "    F.col(\"properties.HOUSING10\").alias(\"HOUSING10\"),\n",
    ").filter(F.col(\"Population\") > 0)  # Exclude zero or negative population\n",
    "\n",
    "# Load Crime Data (Parquet format)\n",
    "crime_df = spark.read.parquet(crime_path)\n",
    "\n",
    "# Create geometry column using ST_Point\n",
    "crime_df = crime_df.withColumn(\"geometry\", F.expr(\"ST_Point(LON, LAT)\")) \\\n",
    "                   .select(\"geometry\")\n",
    "\n",
    "# Load Income Data (CSV format)\n",
    "income_df = spark.read.csv(income_path, header=True, inferSchema=True)\n",
    "income_df = income_df.withColumn(\n",
    "    \"Income\",\n",
    "    F.regexp_replace(F.col(\"Estimated Median Income\"), \"[$,]\", \"\").cast(\"double\")\n",
    ").withColumnRenamed(\"Zip Code\",\"Zip\").drop(\"Estimated Median Income\")\n",
    "\n",
    "census_agg = census_df.groupBy(\"COMM\").agg(\n",
    "    F.sum(\"Population\").alias(\"Total_Population\"),\n",
    "    F.sum(\"HOUSING10\").alias(\"Total_Households\")\n",
    ")\n",
    "# Calculate total income per community (sum of income contributions per zip)\n",
    "income_total = census_df.join(income_df, census_df.ZCTA10 == income_df.Zip, \"inner\") \\\n",
    "    .groupBy(\"COMM\").agg(\n",
    "        F.sum(F.col(\"Income\") * F.col(\"HOUSING10\")).alias(\"Total_Income\")\n",
    "    )\n",
    "\n",
    "# Join census and income data\n",
    "census_income = census_agg.join(income_total, \"COMM\", \"inner\")\n",
    "\n",
    "# Calculate Mean Income Per Person\n",
    "census_income = census_income.withColumn(\n",
    "    \"Mean_Income_Per_Person\",\n",
    "    F.col(\"Total_Income\") / F.col(\"Total_Population\")\n",
    ")\n",
    "\n",
    "# Aggregate crime data\n",
    "crime_agg = crime_df.alias(\"cr\").join(\n",
    "    census_df.alias(\"c\"),\n",
    "    F.expr(\"ST_Within(cr.geometry, c.geometry)\"),\n",
    "    \"inner\"\n",
    ").groupBy(\"c.COMM\").agg(\n",
    "    F.count(\"*\").alias(\"Total_Crimes\")\n",
    ")\n",
    "\n",
    "# Final join for all data\n",
    "final_result = census_income.join(crime_agg, \"COMM\", \"left_outer\") \\\n",
    "    .withColumn(\n",
    "        \"Crime_Per_Person_Ratio\",\n",
    "        F.col(\"Total_Crimes\") / F.col(\"Total_Population\")\n",
    "    )\n",
    "\n",
    "# Replace NULL values with 0 in the columns \"Total_Crimes\" and \"Crime_Per_Person_Ratio\"\n",
    "final_result = final_result.fillna({\n",
    "    \"Total_Crimes\": 0,\n",
    "    \"Crime_Per_Person_Ratio\": 0\n",
    "})\n",
    "\n",
    "# Display final results\n",
    "final_result.select(\n",
    "    \"COMM\",\n",
    "    \"Total_Population\",\n",
    "    \"Mean_Income_Per_Person\",\n",
    "    \"Total_Crimes\",\n",
    "    \"Crime_Per_Person_Ratio\"\n",
    ").show(truncate=False)\n",
    "\n",
    "\n",
    "# # Broadcast Join Hint\n",
    "# broadcast_result = census_income.hint(\"broadcast\").join(crime_agg, \"COMM\", \"left_outer\")\n",
    "\n",
    "# # Explain Broadcast Plan\n",
    "# print(\"Broadcast Join Plan:\")\n",
    "# broadcast_result.explain()\n",
    "\n",
    "# # Merge Join Hint\n",
    "# merge_result = census_income.hint(\"merge\").join(crime_agg, \"COMM\", \"left_outer\")\n",
    "\n",
    "# # Explain Merge Plan\n",
    "# print(\"Merge Join Plan:\")\n",
    "# merge_result.explain()\n",
    "\n",
    "# # Shuffle Hash Join Hint\n",
    "# shuffle_hash_result = census_income.hint(\"shuffle_hash\").join(crime_agg, \"COMM\", \"left_outer\")\n",
    "\n",
    "# # Explain Shuffle Hash Plan\n",
    "# print(\"Shuffle Hash Join Plan:\")\n",
    "# shuffle_hash_result.explain()\n",
    "\n",
    "# # Shuffle Replicate NL Join Hint\n",
    "# shuffle_replicate_result = census_income.hint(\"shuffle_replicate_nl\").join(crime_agg, \"COMM\", \"left_outer\")\n",
    "\n",
    "# # Explain Shuffle Replicate NL Plan\n",
    "# print(\"Shuffle Replicate NL Join Plan:\")\n",
    "# shuffle_replicate_result.explain()\n",
    "\n",
    "# Count total rows in the final result\n",
    "print(\"Total communities:\", final_result.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "816bcea7-85df-4f00-9abc-11651edf33e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Vict Descent` cannot be resolved. Did you mean one of the following? [`COMM`].\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1732639283265_1439/container_1732639283265_1439_01_000001/pyspark.zip/pyspark/sql/dataframe.py\", line 3081, in __getitem__\n",
      "    jc = self._jdf.apply(item)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1732639283265_1439/container_1732639283265_1439_01_000001/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1732639283265_1439/container_1732639283265_1439_01_000001/pyspark.zip/pyspark/errors/exceptions/captured.py\", line 185, in deco\n",
      "    raise converted from None\n",
      "pyspark.errors.exceptions.captured.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Vict Descent` cannot be resolved. Did you mean one of the following? [`COMM`].\n",
      "\n"
     ]
    }
   ],
   "source": [
    "To evaluate the performance of the joins and decide the best-suited one, here’s a consolidated analysis of the **Shuffle Hash Join**, **Broadcast Hash Join**, and **Shuffle Replicate Nested Loop Join** from the physical plans you provided:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Shuffle Hash Join**:\n",
    "- **Description**: \n",
    "  - The **Shuffle Hash Join** partitions the data on both sides of the join condition (`COMM`) and distributes it across the cluster. Each partition processes the join locally.\n",
    "  - It is typically used when both datasets are large but can still fit into memory after partitioning.\n",
    "\n",
    "- **Advantages**:\n",
    "  - Scales well for large datasets.\n",
    "  - Ensures distributed parallelism for performance.\n",
    "\n",
    "- **Disadvantages**:\n",
    "  - Shuffle operations are expensive in terms of network I/O.\n",
    "  - Performance can degrade significantly if the data skew exists (i.e., when certain `COMM` values have significantly more rows than others).\n",
    "\n",
    "- **Best Suited**: \n",
    "  - When both datasets are large and can be efficiently partitioned on the join key.\n",
    "  - Works well for **equi-joins**.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Broadcast Hash Join**:\n",
    "- **Description**: \n",
    "  - The **Broadcast Hash Join** replicates the smaller dataset across all worker nodes. The larger dataset is then scanned, and join conditions are evaluated using the replicated smaller dataset.\n",
    "\n",
    "- **Advantages**:\n",
    "  - Extremely fast for joins where one dataset is small enough to fit in memory.\n",
    "  - Avoids the shuffle cost for the smaller dataset.\n",
    "\n",
    "- **Disadvantages**:\n",
    "  - Memory-intensive, as the smaller dataset must be broadcast to all nodes.\n",
    "  - Not suitable if the smaller dataset cannot fit into memory.\n",
    "\n",
    "- **Best Suited**: \n",
    "  - When one dataset is much smaller than the other (e.g., a lookup table or filtered spatial data).\n",
    "  - Effective for **equi-joins** when broadcast conditions are met.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Shuffle Replicate Nested Loop Join (NL Join)**:\n",
    "- **Description**: \n",
    "  - The **Shuffle Replicate NL Join** replicates one dataset across all partitions of the other and evaluates the join condition for all record pairs. It is typically used for **non-equi joins** or spatial joins involving complex predicates (e.g., `WITHIN`).\n",
    "\n",
    "- **Advantages**:\n",
    "  - Handles complex, non-equi join conditions like spatial predicates efficiently.\n",
    "  - Can process joins when no other join types are applicable.\n",
    "\n",
    "- **Disadvantages**:\n",
    "  - Extremely expensive due to the Cartesian product nature of nested loops.\n",
    "  - Requires significant memory and processing power, especially for large datasets.\n",
    "\n",
    "- **Best Suited**:\n",
    "  - When the join condition involves non-equi predicates (e.g., spatial predicates like `WITHIN` or `OVERLAPS`).\n",
    "  - For datasets where one side can be replicated without memory issues.\n",
    "\n",
    "---\n",
    "\n",
    "### **Consolidated Comparison**:\n",
    "| Join Type                   | Best for                   | Limitations                            | Cost Efficiency |\n",
    "|-----------------------------|----------------------------|----------------------------------------|-----------------|\n",
    "| **Shuffle Hash Join**       | Large datasets, equi-joins | Expensive shuffle, sensitive to skew  | Moderate        |\n",
    "| **Broadcast Hash Join**     | One small, one large set   | Memory constraints, large broadcasts   | High (if feasible) |\n",
    "| **Shuffle Replicate NL Join** | Non-equi or spatial joins | Extremely costly for large datasets    | Low (except spatial) |\n",
    "\n",
    "---\n",
    "\n",
    "### **Which is the Best?**\n",
    "1. **For Equi-Joins**:\n",
    "   - If both datasets are large: **Shuffle Hash Join**.\n",
    "   - If one dataset is small: **Broadcast Hash Join**.\n",
    "\n",
    "2. **For Spatial Joins** (non-equi predicates like `WITHIN`):\n",
    "   - **Shuffle Replicate NL Join** is necessary but should be optimized by:\n",
    "     - Ensuring the replicated dataset is as small as possible.\n",
    "     - Using spatial indexes or specialized libraries (e.g., Sedona) to reduce computation costs.\n",
    "\n",
    "---\n",
    "\n",
    "Given the dataset sizes\n",
    "\n",
    "1. **Income Dataset**: ~13KB (very small).\n",
    "2. **Census Dataset**: ~184MB (moderately large).\n",
    "3. **Crime Dataset**: ~500MB (large, split into two parts).\n",
    "\n",
    "Here’s how this impacts the choice of join strategies for the different datasets:\n",
    "\n",
    "---\n",
    "\n",
    "### **Analysis of Joins**\n",
    "\n",
    "#### **1. Broadcast Hash Join**:\n",
    "- **Best Scenario**:\n",
    "  - **Broadcast the Income Dataset** (~13KB) as it is very small and fits easily in memory.\n",
    "  - This avoids shuffling the larger Census or Crime datasets.\n",
    "- **Usage**:\n",
    "  - Use Broadcast Hash Join for the join between the **Income** dataset and the **Census** dataset.\n",
    "  - Suitable for the equi-join on `COMM` or similar simple keys.\n",
    "\n",
    "#### **2. Shuffle Hash Join**:\n",
    "- **Best Scenario**:\n",
    "  - When joining **Census** (~184MB) with **Crime** (~500MB).\n",
    "  - These datasets are too large to broadcast, so shuffling and partitioning on the join key (`COMM`) ensures distributed computation.\n",
    "- **Usage**:\n",
    "  - Apply for joins between the larger datasets (Census and Crime) where an equi-join is possible.\n",
    "\n",
    "#### **3. Shuffle Replicate NL Join**:\n",
    "- **Best Scenario**:\n",
    "  - Necessary for spatial joins like the **WITHIN** operation between **Census** and **Crime** datasets.\n",
    "  - Since spatial joins typically involve non-equi conditions, NL Join is required.\n",
    "- **Optimizations**:\n",
    "  - Reduce the dataset size before the join:\n",
    "    - Use spatial filtering or pre-aggregation to limit the number of rows in the Census or Crime datasets.\n",
    "    - Partition the Crime data intelligently to minimize replication overhead.\n",
    "\n",
    "---\n",
    "\n",
    "### **Suggested Join Strategy**\n",
    "\n",
    "1. **Income Join (Income + Census)**:\n",
    "   - Use a **Broadcast Hash Join**.\n",
    "   - Broadcast the Income dataset (13KB) and join with the Census dataset (184MB).\n",
    "   - This ensures minimal shuffle and high performance.\n",
    "\n",
    "2. **Census and Crime Data Join**:\n",
    "   - If it's a **spatial join** (e.g., `WITHIN`), use **Shuffle Replicate NL Join**:\n",
    "     - Optimize by pre-aggregating or filtering both datasets to reduce their sizes.\n",
    "     - Use libraries like **Sedona** for spatial indexing and faster computation.\n",
    "   - If it's an **equi-join** on `COMM` or similar, use **Shuffle Hash Join**:\n",
    "     - Partition both datasets on the join key to avoid excessive shuffling.\n",
    "\n",
    "3. **Final Join (All Data Combined)**:\n",
    "   - Perform the joins in stages, starting with smaller datasets (Income + Census), then join the resulting dataset with the larger Crime dataset.\n",
    "   - This hierarchical join approach minimizes intermediate data size and computational overhead.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion**\n",
    "\n",
    "- **Primary Strategy**: Use **Broadcast Hash Join** wherever possible (Income + Census), as the Income dataset is very small.\n",
    "- **Fallback Strategy**: Use **Shuffle Hash Join** for larger datasets (Census + Crime).\n",
    "- **Spatial Joins**: For spatial operations (`WITHIN`), optimize the **Shuffle Replicate NL Join** by reducing data size or using spatial indexes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39be96ea-93d4-4afb-890a-fd292a891cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b742113-d077-4ffe-a489-c114e86cb59f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+------------------+----+\n",
      "|Year|Area       |Closed_Case_Rate  |Rank|\n",
      "+----+-----------+------------------+----+\n",
      "|2010|Rampart    |32.84713448949121 |1   |\n",
      "|2010|Olympic    |31.515289821999087|2   |\n",
      "|2010|Harbor     |29.36028339237341 |3   |\n",
      "|2011|Olympic    |35.040060090135206|1   |\n",
      "|2011|Rampart    |32.4964471814306  |2   |\n",
      "|2011|Harbor     |28.51336246316431 |3   |\n",
      "|2012|Olympic    |34.29708533302119 |1   |\n",
      "|2012|Rampart    |32.46000463714352 |2   |\n",
      "|2012|Harbor     |29.509585848956675|3   |\n",
      "|2013|Olympic    |33.58217940999398 |1   |\n",
      "|2013|Rampart    |32.1060382916053  |2   |\n",
      "|2013|Harbor     |29.723638951488557|3   |\n",
      "|2014|Van Nuys   |32.0215235281705  |1   |\n",
      "|2014|West Valley|31.49754809505847 |2   |\n",
      "|2014|Mission    |31.224939855653567|3   |\n",
      "|2015|Van Nuys   |32.265140677157845|1   |\n",
      "|2015|Mission    |30.463762673676303|2   |\n",
      "|2015|Foothill   |30.353001803658852|3   |\n",
      "|2016|Van Nuys   |32.194518462124094|1   |\n",
      "|2016|West Valley|31.40146437042384 |2   |\n",
      "+----+-----------+------------------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken: 22.39 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query2_DataFrame\") \\\n",
    "    .config(\"spark.executor.instances\", 4) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Load crime data\n",
    "# Load crime data\n",
    "# Load the 2010-2019 crime data\n",
    "crime_df_2010_2019 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", \n",
    "    header=True\n",
    ")\n",
    "\n",
    "# Load the 2020-present crime data\n",
    "crime_df_2020_present = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\", \n",
    "    header=True\n",
    ")\n",
    "\n",
    "crime_df = crime_df_2010_2019.union(crime_df_2020_present)\n",
    "\n",
    "# parquet_path = \"s3://groups-bucket-dblab-905418150721/group24/results/q2_parquet_maindata/part-00000-f07e4ac6-6590-4191-a971-857760026d74-c000.snappy.parquet\"\n",
    "# crime_df = spark.read.parquet(parquet_path)\n",
    "\n",
    "# Clean up the data and filter out cases based on 'Status Desc'\n",
    "# Open cases have 'Status Desc' as 'UNK' or 'InvestCont', others are considered closed\n",
    "crime_df = crime_df.withColumn(\n",
    "    \"Case_Status\",\n",
    "    F.when(F.col(\"Status Desc\").isin(\"UNK\", \"Invest Cont\"), \"Open\").otherwise(\"Closed\")\n",
    ")\n",
    "# crime_df.select(\"Status Desc\", \"Case_Status\").show(10)\n",
    "# Extract the year from 'Date Rptd' (timestamp column)\n",
    "crime_df = crime_df.withColumn(\"Year\", F.year(F.to_timestamp(\"DATE OCC\", \"MM/dd/yyyy hh:mm:ss a\")))\n",
    "\n",
    "# Group by year and area to calculate the closed case rate\n",
    "year_area_group = crime_df.groupBy(\n",
    "    F.col(\"Year\"),\n",
    "    F.col(\"AREA NAME\").alias(\"Area\")\n",
    ").agg(\n",
    "    F.sum(F.when(F.col(\"Case_Status\") == \"Closed\", 1).otherwise(0)).alias(\"Closed_Cases\"),\n",
    "    F.count(F.col(\"Case_Status\")).alias(\"Total_Cases\")\n",
    ")\n",
    "\n",
    "# Calculate the closed case rate\n",
    "year_area_group = year_area_group.withColumn(\n",
    "    \"Closed_Case_Rate\",\n",
    "    (F.col(\"Closed_Cases\") / F.col(\"Total_Cases\")) * 100\n",
    ")\n",
    "\n",
    "# Rank areas within each year based on the closed case rate\n",
    "window_spec = Window.partitionBy(\"Year\").orderBy(F.col(\"Closed_Case_Rate\").desc())\n",
    "\n",
    "ranked_df = year_area_group.withColumn(\n",
    "    \"Rank\",\n",
    "    F.row_number().over(window_spec)\n",
    ")\n",
    "\n",
    "# Filter the top 3 areas for each year\n",
    "top_3_areas_df = ranked_df.filter(F.col(\"Rank\") <= 3)\n",
    "\n",
    "# Show the results\n",
    "top_3_areas_df.select(\"Year\", \"Area\", \"Closed_Case_Rate\", \"Rank\") \\\n",
    "    .orderBy(\"Year\", \"Rank\") \\\n",
    "    .show(truncate=False)\n",
    "\n",
    "# Stop timing and print out the execution duration\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Write results to parquet\n",
    "group_number = \"24\"\n",
    "s3_path = \"s3://groups-bucket-dblab-905418150721/group\"+group_number+\"/results/\"\n",
    "\n",
    "top_3_areas_df.write.mode(\"overwrite\").parquet(s3_path + \"q2_dataframe_output\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2794f937-1910-42f1-97a4-bf80eed6ca6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

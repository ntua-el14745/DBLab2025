{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "730c9c0b-1484-4a22-b34d-f38be9587aba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+----------------+----------------------+------------+----------------------+\n",
      "|COMM                 |Total_Population|Mean_Income_Per_Person|Total_Crimes|Crime_Per_Person_Ratio|\n",
      "+---------------------+----------------+----------------------+------------+----------------------+\n",
      "|Culver City          |38883           |33644.97549057429     |345         |0.00887277216264177   |\n",
      "|North Lancaster      |1151            |19097.289313640315    |0           |0.0                   |\n",
      "|Rosewood/East Gardena|1164            |16165.823024054982    |101         |0.08676975945017182   |\n",
      "|East Rancho Dominguez|15135           |8830.036339610175     |0           |0.0                   |\n",
      "|Toluca Terrace       |1301            |20167.531898539586    |289         |0.22213681783243658   |\n",
      "|Elysian Park         |5267            |13865.32770077843     |3191        |0.6058477311562559    |\n",
      "|Longwood             |4210            |13420.052256532066    |3062        |0.7273159144893112    |\n",
      "|Pico Rivera          |62942           |15157.884512726001    |2           |3.177528518318452E-5  |\n",
      "|Malibu               |12645           |67046.98133649664     |1           |7.908264136022143E-5  |\n",
      "|Green Meadows        |19821           |8027.096412895414     |21961       |1.1079662983704153    |\n",
      "|Hacienda Heights     |53594           |24046.483673545546    |0           |0.0                   |\n",
      "|Cadillac-Corning     |6665            |19572.784696174043    |3877        |0.581695423855964     |\n",
      "|West Puente Valley   |9657            |12527.325566946256    |0           |0.0                   |\n",
      "|Montebello           |62500           |14514.115728          |6           |9.6E-5                |\n",
      "|Mid-city             |14339           |21734.64899923286     |10190       |0.7106492781923426    |\n",
      "|Lake Manor           |1600            |33720.028125          |0           |0.0                   |\n",
      "|Hawaiian Gardens     |14254           |9911.840535989897     |0           |0.0                   |\n",
      "|Lincoln Heights      |31144           |10767.81132802466     |15999       |0.5137105060364757    |\n",
      "|Westlake Village     |8270            |42843.458524788395    |1           |1.2091898428053205E-4 |\n",
      "|Van Nuys             |86019           |14488.189551145677    |67745       |0.787558562643137     |\n",
      "+---------------------+----------------+----------------------+------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Broadcast Join Plan:\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [COMM#1753, Total_Population#1897L, Total_Households#1899L, Total_Income#1928, Mean_Income_Per_Person#1945, Total_Crimes#1970L]\n",
      "   +- SortMergeJoin [COMM#1753], [COMM#2081], LeftOuter\n",
      "      :- Project [COMM#1753, Total_Population#1897L, Total_Households#1899L, Total_Income#1928, (Total_Income#1928 / cast(Total_Population#1897L as double)) AS Mean_Income_Per_Person#1945]\n",
      "      :  +- SortMergeJoin [COMM#1753], [COMM#1937], Inner\n",
      "      :     :- Sort [COMM#1753 ASC NULLS FIRST], false, 0\n",
      "      :     :  +- HashAggregate(keys=[COMM#1753], functions=[sum(Population#1752L), sum(HOUSING10#1755L)], schema specialized)\n",
      "      :     :     +- Exchange hashpartitioning(COMM#1753, 1000), ENSURE_REQUIREMENTS, [plan_id=5610]\n",
      "      :     :        +- HashAggregate(keys=[COMM#1753], functions=[partial_sum(Population#1752L), partial_sum(HOUSING10#1755L)], schema specialized)\n",
      "      :     :           +- Project [features#1742.properties.POP_2010 AS Population#1752L, features#1742.properties.COMM AS COMM#1753, features#1742.properties.HOUSING10 AS HOUSING10#1755L]\n",
      "      :     :              +- Filter ((isnotnull(features#1742.properties.POP_2010) AND (features#1742.properties.POP_2010 > 0)) AND isnotnull(features#1742.properties.COMM))\n",
      "      :     :                 +- Generate explode(features#1734), false, [features#1742]\n",
      "      :     :                    +- Filter ((size(features#1734, true) > 0) AND isnotnull(features#1734))\n",
      "      :     :                       +- FileScan geojson [features#1734] Batched: false, DataFilters: [(size(features#1734, true) > 0), isnotnull(features#1734)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "      :     +- Sort [COMM#1937 ASC NULLS FIRST], false, 0\n",
      "      :        +- HashAggregate(keys=[COMM#1937], functions=[sum((Income#1877 * cast(HOUSING10#1939L as double)))], schema specialized)\n",
      "      :           +- Exchange hashpartitioning(COMM#1937, 1000), ENSURE_REQUIREMENTS, [plan_id=5649]\n",
      "      :              +- HashAggregate(keys=[COMM#1937], functions=[partial_sum((Income#1877 * cast(HOUSING10#1939L as double)))], schema specialized)\n",
      "      :                 +- Project [COMM#1937, HOUSING10#1939L, Income#1877]\n",
      "      :                    +- BroadcastHashJoin [cast(ZCTA10#1935 as int)], [Zip#1883], Inner, BuildRight, false\n",
      "      :                       :- Project [features#1742.properties.ZCTA10 AS ZCTA10#1935, features#1742.properties.COMM AS COMM#1937, features#1742.properties.HOUSING10 AS HOUSING10#1939L]\n",
      "      :                       :  +- Filter (((isnotnull(features#1742.properties.POP_2010) AND (features#1742.properties.POP_2010 > 0)) AND isnotnull(features#1742.properties.ZCTA10)) AND isnotnull(features#1742.properties.COMM))\n",
      "      :                       :     +- Generate explode(features#1932), false, [features#1742]\n",
      "      :                       :        +- Filter ((size(features#1932, true) > 0) AND isnotnull(features#1932))\n",
      "      :                       :           +- FileScan geojson [features#1932] Batched: false, DataFilters: [(size(features#1932, true) > 0), isnotnull(features#1932)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "      :                       +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=5612]\n",
      "      :                          +- Project [Zip Code#1871 AS Zip#1883, cast(regexp_replace(Estimated Median Income#1873, [$,], , 1) as double) AS Income#1877]\n",
      "      :                             +- Filter isnotnull(Zip Code#1871)\n",
      "      :                                +- FileScan csv [Zip Code#1871,Estimated Median Income#1873] Batched: false, DataFilters: [isnotnull(Zip Code#1871)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:int,Estimated Median Income:string>\n",
      "      +- Sort [COMM#2081 ASC NULLS FIRST], false, 0\n",
      "         +- HashAggregate(keys=[COMM#2081], functions=[count(1)], schema specialized)\n",
      "            +- Exchange hashpartitioning(COMM#2081, 1000), ENSURE_REQUIREMENTS, [plan_id=5625]\n",
      "               +- HashAggregate(keys=[COMM#2081], functions=[partial_count(1)], schema specialized)\n",
      "                  +- Project [COMM#2081]\n",
      "                     +- RangeJoin geometry#1821: geometry, geometry#2082: geometry, WITHIN\n",
      "                        :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geometry#1821]\n",
      "                        :  +- Filter isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  )\n",
      "                        :     +- FileScan parquet [LAT#1791,LON#1792] Batched: true, DataFilters: [isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  )], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3://groups-bucket-dblab-905418150721/group24/results/q2_parquet_maind..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<LAT:string,LON:string>\n",
      "                        +- Project [features#1742.properties.COMM AS COMM#2081, features#1742.geometry AS geometry#2082]\n",
      "                           +- Filter ((isnotnull(features#1742.properties.POP_2010) AND (features#1742.properties.POP_2010 > 0)) AND (isnotnull(features#1742.geometry) AND isnotnull(features#1742.properties.COMM)))\n",
      "                              +- Generate explode(features#2076), false, [features#1742]\n",
      "                                 +- Filter ((size(features#2076, true) > 0) AND isnotnull(features#2076))\n",
      "                                    +- FileScan geojson [features#2076] Batched: false, DataFilters: [(size(features#2076, true) > 0), isnotnull(features#2076)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "\n",
      "\n",
      "Merge Join Plan:\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [COMM#1753, Total_Population#1897L, Total_Households#1899L, Total_Income#1928, Mean_Income_Per_Person#1945, Total_Crimes#1970L]\n",
      "   +- SortMergeJoin [COMM#1753], [COMM#2109], LeftOuter\n",
      "      :- Project [COMM#1753, Total_Population#1897L, Total_Households#1899L, Total_Income#1928, (Total_Income#1928 / cast(Total_Population#1897L as double)) AS Mean_Income_Per_Person#1945]\n",
      "      :  +- SortMergeJoin [COMM#1753], [COMM#1937], Inner\n",
      "      :     :- Sort [COMM#1753 ASC NULLS FIRST], false, 0\n",
      "      :     :  +- HashAggregate(keys=[COMM#1753], functions=[sum(Population#1752L), sum(HOUSING10#1755L)], schema specialized)\n",
      "      :     :     +- Exchange hashpartitioning(COMM#1753, 1000), ENSURE_REQUIREMENTS, [plan_id=5975]\n",
      "      :     :        +- HashAggregate(keys=[COMM#1753], functions=[partial_sum(Population#1752L), partial_sum(HOUSING10#1755L)], schema specialized)\n",
      "      :     :           +- Project [features#1742.properties.POP_2010 AS Population#1752L, features#1742.properties.COMM AS COMM#1753, features#1742.properties.HOUSING10 AS HOUSING10#1755L]\n",
      "      :     :              +- Filter ((isnotnull(features#1742.properties.POP_2010) AND (features#1742.properties.POP_2010 > 0)) AND isnotnull(features#1742.properties.COMM))\n",
      "      :     :                 +- Generate explode(features#1734), false, [features#1742]\n",
      "      :     :                    +- Filter ((size(features#1734, true) > 0) AND isnotnull(features#1734))\n",
      "      :     :                       +- FileScan geojson [features#1734] Batched: false, DataFilters: [(size(features#1734, true) > 0), isnotnull(features#1734)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "      :     +- Sort [COMM#1937 ASC NULLS FIRST], false, 0\n",
      "      :        +- HashAggregate(keys=[COMM#1937], functions=[sum((Income#1877 * cast(HOUSING10#1939L as double)))], schema specialized)\n",
      "      :           +- Exchange hashpartitioning(COMM#1937, 1000), ENSURE_REQUIREMENTS, [plan_id=6014]\n",
      "      :              +- HashAggregate(keys=[COMM#1937], functions=[partial_sum((Income#1877 * cast(HOUSING10#1939L as double)))], schema specialized)\n",
      "      :                 +- Project [COMM#1937, HOUSING10#1939L, Income#1877]\n",
      "      :                    +- BroadcastHashJoin [cast(ZCTA10#1935 as int)], [Zip#1883], Inner, BuildRight, false\n",
      "      :                       :- Project [features#1742.properties.ZCTA10 AS ZCTA10#1935, features#1742.properties.COMM AS COMM#1937, features#1742.properties.HOUSING10 AS HOUSING10#1939L]\n",
      "      :                       :  +- Filter (((isnotnull(features#1742.properties.POP_2010) AND (features#1742.properties.POP_2010 > 0)) AND isnotnull(features#1742.properties.ZCTA10)) AND isnotnull(features#1742.properties.COMM))\n",
      "      :                       :     +- Generate explode(features#1932), false, [features#1742]\n",
      "      :                       :        +- Filter ((size(features#1932, true) > 0) AND isnotnull(features#1932))\n",
      "      :                       :           +- FileScan geojson [features#1932] Batched: false, DataFilters: [(size(features#1932, true) > 0), isnotnull(features#1932)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "      :                       +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=5977]\n",
      "      :                          +- Project [Zip Code#1871 AS Zip#1883, cast(regexp_replace(Estimated Median Income#1873, [$,], , 1) as double) AS Income#1877]\n",
      "      :                             +- Filter isnotnull(Zip Code#1871)\n",
      "      :                                +- FileScan csv [Zip Code#1871,Estimated Median Income#1873] Batched: false, DataFilters: [isnotnull(Zip Code#1871)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:int,Estimated Median Income:string>\n",
      "      +- Sort [COMM#2109 ASC NULLS FIRST], false, 0\n",
      "         +- HashAggregate(keys=[COMM#2109], functions=[count(1)], schema specialized)\n",
      "            +- Exchange hashpartitioning(COMM#2109, 1000), ENSURE_REQUIREMENTS, [plan_id=5990]\n",
      "               +- HashAggregate(keys=[COMM#2109], functions=[partial_count(1)], schema specialized)\n",
      "                  +- Project [COMM#2109]\n",
      "                     +- RangeJoin geometry#1821: geometry, geometry#2110: geometry, WITHIN\n",
      "                        :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geometry#1821]\n",
      "                        :  +- Filter isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  )\n",
      "                        :     +- FileScan parquet [LAT#1791,LON#1792] Batched: true, DataFilters: [isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  )], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3://groups-bucket-dblab-905418150721/group24/results/q2_parquet_maind..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<LAT:string,LON:string>\n",
      "                        +- Project [features#1742.properties.COMM AS COMM#2109, features#1742.geometry AS geometry#2110]\n",
      "                           +- Filter ((isnotnull(features#1742.properties.POP_2010) AND (features#1742.properties.POP_2010 > 0)) AND (isnotnull(features#1742.geometry) AND isnotnull(features#1742.properties.COMM)))\n",
      "                              +- Generate explode(features#2104), false, [features#1742]\n",
      "                                 +- Filter ((size(features#2104, true) > 0) AND isnotnull(features#2104))\n",
      "                                    +- FileScan geojson [features#2104] Batched: false, DataFilters: [(size(features#2104, true) > 0), isnotnull(features#2104)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "\n",
      "\n",
      "Shuffle Hash Join Plan:\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [COMM#1753, Total_Population#1897L, Total_Households#1899L, Total_Income#1928, Mean_Income_Per_Person#1945, Total_Crimes#1970L]\n",
      "   +- ShuffledHashJoin [COMM#1753], [COMM#2135], LeftOuter, BuildLeft\n",
      "      :- Project [COMM#1753, Total_Population#1897L, Total_Households#1899L, Total_Income#1928, (Total_Income#1928 / cast(Total_Population#1897L as double)) AS Mean_Income_Per_Person#1945]\n",
      "      :  +- SortMergeJoin [COMM#1753], [COMM#1937], Inner\n",
      "      :     :- Sort [COMM#1753 ASC NULLS FIRST], false, 0\n",
      "      :     :  +- HashAggregate(keys=[COMM#1753], functions=[sum(Population#1752L), sum(HOUSING10#1755L)], schema specialized)\n",
      "      :     :     +- Exchange hashpartitioning(COMM#1753, 1000), ENSURE_REQUIREMENTS, [plan_id=6340]\n",
      "      :     :        +- HashAggregate(keys=[COMM#1753], functions=[partial_sum(Population#1752L), partial_sum(HOUSING10#1755L)], schema specialized)\n",
      "      :     :           +- Project [features#1742.properties.POP_2010 AS Population#1752L, features#1742.properties.COMM AS COMM#1753, features#1742.properties.HOUSING10 AS HOUSING10#1755L]\n",
      "      :     :              +- Filter ((isnotnull(features#1742.properties.POP_2010) AND (features#1742.properties.POP_2010 > 0)) AND isnotnull(features#1742.properties.COMM))\n",
      "      :     :                 +- Generate explode(features#1734), false, [features#1742]\n",
      "      :     :                    +- Filter ((size(features#1734, true) > 0) AND isnotnull(features#1734))\n",
      "      :     :                       +- FileScan geojson [features#1734] Batched: false, DataFilters: [(size(features#1734, true) > 0), isnotnull(features#1734)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "      :     +- Sort [COMM#1937 ASC NULLS FIRST], false, 0\n",
      "      :        +- HashAggregate(keys=[COMM#1937], functions=[sum((Income#1877 * cast(HOUSING10#1939L as double)))], schema specialized)\n",
      "      :           +- Exchange hashpartitioning(COMM#1937, 1000), ENSURE_REQUIREMENTS, [plan_id=6377]\n",
      "      :              +- HashAggregate(keys=[COMM#1937], functions=[partial_sum((Income#1877 * cast(HOUSING10#1939L as double)))], schema specialized)\n",
      "      :                 +- Project [COMM#1937, HOUSING10#1939L, Income#1877]\n",
      "      :                    +- BroadcastHashJoin [cast(ZCTA10#1935 as int)], [Zip#1883], Inner, BuildRight, false\n",
      "      :                       :- Project [features#1742.properties.ZCTA10 AS ZCTA10#1935, features#1742.properties.COMM AS COMM#1937, features#1742.properties.HOUSING10 AS HOUSING10#1939L]\n",
      "      :                       :  +- Filter (((isnotnull(features#1742.properties.POP_2010) AND (features#1742.properties.POP_2010 > 0)) AND isnotnull(features#1742.properties.ZCTA10)) AND isnotnull(features#1742.properties.COMM))\n",
      "      :                       :     +- Generate explode(features#1932), false, [features#1742]\n",
      "      :                       :        +- Filter ((size(features#1932, true) > 0) AND isnotnull(features#1932))\n",
      "      :                       :           +- FileScan geojson [features#1932] Batched: false, DataFilters: [(size(features#1932, true) > 0), isnotnull(features#1932)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "      :                       +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=6342]\n",
      "      :                          +- Project [Zip Code#1871 AS Zip#1883, cast(regexp_replace(Estimated Median Income#1873, [$,], , 1) as double) AS Income#1877]\n",
      "      :                             +- Filter isnotnull(Zip Code#1871)\n",
      "      :                                +- FileScan csv [Zip Code#1871,Estimated Median Income#1873] Batched: false, DataFilters: [isnotnull(Zip Code#1871)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:int,Estimated Median Income:string>\n",
      "      +- HashAggregate(keys=[COMM#2135], functions=[count(1)], schema specialized)\n",
      "         +- Exchange hashpartitioning(COMM#2135, 1000), ENSURE_REQUIREMENTS, [plan_id=6355]\n",
      "            +- HashAggregate(keys=[COMM#2135], functions=[partial_count(1)], schema specialized)\n",
      "               +- Project [COMM#2135]\n",
      "                  +- RangeJoin geometry#1821: geometry, geometry#2136: geometry, WITHIN\n",
      "                     :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geometry#1821]\n",
      "                     :  +- Filter isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  )\n",
      "                     :     +- FileScan parquet [LAT#1791,LON#1792] Batched: true, DataFilters: [isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  )], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3://groups-bucket-dblab-905418150721/group24/results/q2_parquet_maind..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<LAT:string,LON:string>\n",
      "                     +- Project [features#1742.properties.COMM AS COMM#2135, features#1742.geometry AS geometry#2136]\n",
      "                        +- Filter ((isnotnull(features#1742.properties.POP_2010) AND (features#1742.properties.POP_2010 > 0)) AND (isnotnull(features#1742.geometry) AND isnotnull(features#1742.properties.COMM)))\n",
      "                           +- Generate explode(features#2130), false, [features#1742]\n",
      "                              +- Filter ((size(features#2130, true) > 0) AND isnotnull(features#2130))\n",
      "                                 +- FileScan geojson [features#2130] Batched: false, DataFilters: [(size(features#2130, true) > 0), isnotnull(features#2130)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "\n",
      "\n",
      "Shuffle Replicate NL Join Plan:\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [COMM#1753, Total_Population#1897L, Total_Households#1899L, Total_Income#1928, Mean_Income_Per_Person#1945, Total_Crimes#1970L]\n",
      "   +- SortMergeJoin [COMM#1753], [COMM#2161], LeftOuter\n",
      "      :- Project [COMM#1753, Total_Population#1897L, Total_Households#1899L, Total_Income#1928, (Total_Income#1928 / cast(Total_Population#1897L as double)) AS Mean_Income_Per_Person#1945]\n",
      "      :  +- SortMergeJoin [COMM#1753], [COMM#1937], Inner\n",
      "      :     :- Sort [COMM#1753 ASC NULLS FIRST], false, 0\n",
      "      :     :  +- HashAggregate(keys=[COMM#1753], functions=[sum(Population#1752L), sum(HOUSING10#1755L)], schema specialized)\n",
      "      :     :     +- Exchange hashpartitioning(COMM#1753, 1000), ENSURE_REQUIREMENTS, [plan_id=6701]\n",
      "      :     :        +- HashAggregate(keys=[COMM#1753], functions=[partial_sum(Population#1752L), partial_sum(HOUSING10#1755L)], schema specialized)\n",
      "      :     :           +- Project [features#1742.properties.POP_2010 AS Population#1752L, features#1742.properties.COMM AS COMM#1753, features#1742.properties.HOUSING10 AS HOUSING10#1755L]\n",
      "      :     :              +- Filter ((isnotnull(features#1742.properties.POP_2010) AND (features#1742.properties.POP_2010 > 0)) AND isnotnull(features#1742.properties.COMM))\n",
      "      :     :                 +- Generate explode(features#1734), false, [features#1742]\n",
      "      :     :                    +- Filter ((size(features#1734, true) > 0) AND isnotnull(features#1734))\n",
      "      :     :                       +- FileScan geojson [features#1734] Batched: false, DataFilters: [(size(features#1734, true) > 0), isnotnull(features#1734)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "      :     +- Sort [COMM#1937 ASC NULLS FIRST], false, 0\n",
      "      :        +- HashAggregate(keys=[COMM#1937], functions=[sum((Income#1877 * cast(HOUSING10#1939L as double)))], schema specialized)\n",
      "      :           +- Exchange hashpartitioning(COMM#1937, 1000), ENSURE_REQUIREMENTS, [plan_id=6740]\n",
      "      :              +- HashAggregate(keys=[COMM#1937], functions=[partial_sum((Income#1877 * cast(HOUSING10#1939L as double)))], schema specialized)\n",
      "      :                 +- Project [COMM#1937, HOUSING10#1939L, Income#1877]\n",
      "      :                    +- BroadcastHashJoin [cast(ZCTA10#1935 as int)], [Zip#1883], Inner, BuildRight, false\n",
      "      :                       :- Project [features#1742.properties.ZCTA10 AS ZCTA10#1935, features#1742.properties.COMM AS COMM#1937, features#1742.properties.HOUSING10 AS HOUSING10#1939L]\n",
      "      :                       :  +- Filter (((isnotnull(features#1742.properties.POP_2010) AND (features#1742.properties.POP_2010 > 0)) AND isnotnull(features#1742.properties.ZCTA10)) AND isnotnull(features#1742.properties.COMM))\n",
      "      :                       :     +- Generate explode(features#1932), false, [features#1742]\n",
      "      :                       :        +- Filter ((size(features#1932, true) > 0) AND isnotnull(features#1932))\n",
      "      :                       :           +- FileScan geojson [features#1932] Batched: false, DataFilters: [(size(features#1932, true) > 0), isnotnull(features#1932)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "      :                       +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=6703]\n",
      "      :                          +- Project [Zip Code#1871 AS Zip#1883, cast(regexp_replace(Estimated Median Income#1873, [$,], , 1) as double) AS Income#1877]\n",
      "      :                             +- Filter isnotnull(Zip Code#1871)\n",
      "      :                                +- FileScan csv [Zip Code#1871,Estimated Median Income#1873] Batched: false, DataFilters: [isnotnull(Zip Code#1871)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:int,Estimated Median Income:string>\n",
      "      +- Sort [COMM#2161 ASC NULLS FIRST], false, 0\n",
      "         +- HashAggregate(keys=[COMM#2161], functions=[count(1)], schema specialized)\n",
      "            +- Exchange hashpartitioning(COMM#2161, 1000), ENSURE_REQUIREMENTS, [plan_id=6716]\n",
      "               +- HashAggregate(keys=[COMM#2161], functions=[partial_count(1)], schema specialized)\n",
      "                  +- Project [COMM#2161]\n",
      "                     +- RangeJoin geometry#1821: geometry, geometry#2162: geometry, WITHIN\n",
      "                        :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geometry#1821]\n",
      "                        :  +- Filter isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  )\n",
      "                        :     +- FileScan parquet [LAT#1791,LON#1792] Batched: true, DataFilters: [isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  )], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3://groups-bucket-dblab-905418150721/group24/results/q2_parquet_maind..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<LAT:string,LON:string>\n",
      "                        +- Project [features#1742.properties.COMM AS COMM#2161, features#1742.geometry AS geometry#2162]\n",
      "                           +- Filter ((isnotnull(features#1742.properties.POP_2010) AND (features#1742.properties.POP_2010 > 0)) AND (isnotnull(features#1742.geometry) AND isnotnull(features#1742.properties.COMM)))\n",
      "                              +- Generate explode(features#2156), false, [features#1742]\n",
      "                                 +- Filter ((size(features#2156, true) > 0) AND isnotnull(features#2156))\n",
      "                                    +- FileScan geojson [features#2156] Batched: false, DataFilters: [(size(features#2156, true) > 0), isnotnull(features#2156)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "\n",
      "\n",
      "Total communities: 319"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from sedona.register.geo_registrator import SedonaRegistrator\n",
    "from sedona.spark import SedonaContext\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Initialize Spark Session and Sedona Context\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CrimeAnalysisSpatialJoinSQL\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.sedona.sql.SedonaSqlExtensions\") \\\n",
    "    .config(\"sedona.global.charset\", \"utf8\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sedona = SedonaContext.create(spark)\n",
    "\n",
    "# Load datasets\n",
    "census_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "income_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\"\n",
    "crime_path = \"s3://groups-bucket-dblab-905418150721/group24/results/q2_parquet_maindata/\"\n",
    "\n",
    "# Load Census Data (GeoJSON format)\n",
    "census_df = sedona.read.format(\"geojson\") \\\n",
    "    .option(\"multiLine\", \"true\").load(census_path) \\\n",
    "    .selectExpr(\"explode(features) as features\") \\\n",
    "    .select(\"features.*\")\n",
    "\n",
    "# Flatten the GeoJSON structure and filter valid populations\n",
    "census_df = census_df.select(\n",
    "    F.col(\"properties.ZCTA10\").alias(\"ZCTA10\"),\n",
    "    F.col(\"properties.POP_2010\").alias(\"Population\"),\n",
    "    F.col(\"properties.COMM\").alias(\"COMM\"),\n",
    "    F.col(\"geometry\").alias(\"geometry\"),\n",
    "    F.col(\"properties.HOUSING10\").alias(\"HOUSING10\"),\n",
    ").filter(F.col(\"Population\") > 0)  # Exclude zero or negative population\n",
    "\n",
    "# Load Crime Data (Parquet format)\n",
    "crime_df = spark.read.parquet(crime_path)\n",
    "\n",
    "# Create geometry column using ST_Point\n",
    "crime_df = crime_df.withColumn(\"geometry\", F.expr(\"ST_Point(LON, LAT)\")) \\\n",
    "                   .select(\"geometry\")\n",
    "\n",
    "# Load Income Data (CSV format)\n",
    "income_df = spark.read.csv(income_path, header=True, inferSchema=True)\n",
    "income_df = income_df.withColumn(\n",
    "    \"Income\",\n",
    "    F.regexp_replace(F.col(\"Estimated Median Income\"), \"[$,]\", \"\").cast(\"double\")\n",
    ").withColumnRenamed(\"Zip Code\",\"Zip\").drop(\"Estimated Median Income\")\n",
    "\n",
    "census_agg = census_df.groupBy(\"COMM\").agg(\n",
    "    F.sum(\"Population\").alias(\"Total_Population\"),\n",
    "    F.sum(\"HOUSING10\").alias(\"Total_Households\")\n",
    ")\n",
    "# Calculate total income per community (sum of income contributions per zip)\n",
    "income_total = census_df.join(income_df, census_df.ZCTA10 == income_df.Zip, \"inner\") \\\n",
    "    .groupBy(\"COMM\").agg(\n",
    "        F.sum(F.col(\"Income\") * F.col(\"HOUSING10\")).alias(\"Total_Income\")\n",
    "    )\n",
    "\n",
    "# Join census and income data\n",
    "census_income = census_agg.join(income_total, \"COMM\", \"inner\")\n",
    "\n",
    "# Calculate Mean Income Per Person\n",
    "census_income = census_income.withColumn(\n",
    "    \"Mean_Income_Per_Person\",\n",
    "    F.col(\"Total_Income\") / F.col(\"Total_Population\")\n",
    ")\n",
    "\n",
    "# Aggregate crime data\n",
    "crime_agg = crime_df.alias(\"cr\").join(\n",
    "    census_df.alias(\"c\"),\n",
    "    F.expr(\"ST_Within(cr.geometry, c.geometry)\"),\n",
    "    \"inner\"\n",
    ").groupBy(\"c.COMM\").agg(\n",
    "    F.count(\"*\").alias(\"Total_Crimes\")\n",
    ")\n",
    "\n",
    "# Final join for all data\n",
    "final_result = census_income.join(crime_agg, \"COMM\", \"left_outer\") \\\n",
    "    .withColumn(\n",
    "        \"Crime_Per_Person_Ratio\",\n",
    "        F.col(\"Total_Crimes\") / F.col(\"Total_Population\")\n",
    "    )\n",
    "\n",
    "# Replace NULL values with 0 in the columns \"Total_Crimes\" and \"Crime_Per_Person_Ratio\"\n",
    "final_result = final_result.fillna({\n",
    "    \"Total_Crimes\": 0,\n",
    "    \"Crime_Per_Person_Ratio\": 0\n",
    "})\n",
    "\n",
    "# Display final results\n",
    "final_result.select(\n",
    "    \"COMM\",\n",
    "    \"Total_Population\",\n",
    "    \"Mean_Income_Per_Person\",\n",
    "    \"Total_Crimes\",\n",
    "    \"Crime_Per_Person_Ratio\"\n",
    ").show(truncate=False)\n",
    "\n",
    "\n",
    "# Broadcast Join Hint\n",
    "broadcast_result = census_income.hint(\"broadcast\").join(crime_agg, \"COMM\", \"left_outer\")\n",
    "\n",
    "# Explain Broadcast Plan\n",
    "print(\"Broadcast Join Plan:\")\n",
    "broadcast_result.explain()\n",
    "\n",
    "# Merge Join Hint\n",
    "merge_result = census_income.hint(\"merge\").join(crime_agg, \"COMM\", \"left_outer\")\n",
    "\n",
    "# Explain Merge Plan\n",
    "print(\"Merge Join Plan:\")\n",
    "merge_result.explain()\n",
    "\n",
    "# Shuffle Hash Join Hint\n",
    "shuffle_hash_result = census_income.hint(\"shuffle_hash\").join(crime_agg, \"COMM\", \"left_outer\")\n",
    "\n",
    "# Explain Shuffle Hash Plan\n",
    "print(\"Shuffle Hash Join Plan:\")\n",
    "shuffle_hash_result.explain()\n",
    "\n",
    "# Shuffle Replicate NL Join Hint\n",
    "shuffle_replicate_result = census_income.hint(\"shuffle_replicate_nl\").join(crime_agg, \"COMM\", \"left_outer\")\n",
    "\n",
    "# Explain Shuffle Replicate NL Plan\n",
    "print(\"Shuffle Replicate NL Join Plan:\")\n",
    "shuffle_replicate_result.explain()\n",
    "\n",
    "# Count total rows in the final result\n",
    "print(\"Total communities:\", final_result.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816bcea7-85df-4f00-9abc-11651edf33e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

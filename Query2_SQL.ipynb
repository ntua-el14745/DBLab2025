{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b742113-d077-4ffe-a489-c114e86cb59f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+------------------+----+\n",
      "|Year|Area       |Closed_Case_Rate  |Rank|\n",
      "+----+-----------+------------------+----+\n",
      "|2010|Rampart    |32.947355855318136|1   |\n",
      "|2010|Olympic    |31.962706191728422|2   |\n",
      "|2010|Harbor     |29.63203463203463 |3   |\n",
      "|2011|Olympic    |35.212167689161554|1   |\n",
      "|2011|Rampart    |32.511779630300836|2   |\n",
      "|2011|Harbor     |28.652205202015008|3   |\n",
      "|2012|Olympic    |34.414818310523835|1   |\n",
      "|2012|Rampart    |32.9464181029429  |2   |\n",
      "|2012|Harbor     |29.815133276010318|3   |\n",
      "|2013|Olympic    |33.52812271731191 |1   |\n",
      "|2013|Rampart    |32.08287360549221 |2   |\n",
      "|2013|Harbor     |29.16422459266206 |3   |\n",
      "|2014|Van Nuys   |31.80567315834039 |1   |\n",
      "|2014|West Valley|31.31198995605775 |2   |\n",
      "|2014|Mission    |31.16279069767442 |3   |\n",
      "|2015|Van Nuys   |32.64134698172773 |1   |\n",
      "|2015|West Valley|30.27597402597403 |2   |\n",
      "|2015|Mission    |30.179460678380153|3   |\n",
      "|2016|Van Nuys   |31.880755720117726|1   |\n",
      "|2016|West Valley|31.54798761609907 |2   |\n",
      "+----+-----------+------------------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----+-----------+------------------+----+\n",
      "|Year|Area       |Closed_Case_Rate  |Rank|\n",
      "+----+-----------+------------------+----+\n",
      "|2010|Rampart    |32.947355855318136|1   |\n",
      "|2010|Olympic    |31.962706191728422|2   |\n",
      "|2010|Harbor     |29.63203463203463 |3   |\n",
      "|2011|Olympic    |35.212167689161554|1   |\n",
      "|2011|Rampart    |32.511779630300836|2   |\n",
      "|2011|Harbor     |28.652205202015008|3   |\n",
      "|2012|Olympic    |34.414818310523835|1   |\n",
      "|2012|Rampart    |32.9464181029429  |2   |\n",
      "|2012|Harbor     |29.815133276010318|3   |\n",
      "|2013|Olympic    |33.52812271731191 |1   |\n",
      "|2013|Rampart    |32.08287360549221 |2   |\n",
      "|2013|Harbor     |29.16422459266206 |3   |\n",
      "|2014|Van Nuys   |31.80567315834039 |1   |\n",
      "|2014|West Valley|31.31198995605775 |2   |\n",
      "|2014|Mission    |31.16279069767442 |3   |\n",
      "|2015|Van Nuys   |32.64134698172773 |1   |\n",
      "|2015|West Valley|30.27597402597403 |2   |\n",
      "|2015|Mission    |30.179460678380153|3   |\n",
      "|2016|Van Nuys   |31.880755720117726|1   |\n",
      "|2016|West Valley|31.54798761609907 |2   |\n",
      "+----+-----------+------------------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken: 27.15 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import time\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Query2_SQL\") \\\n",
    "    .config(\"spark.executor.instances\", 4) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Load crime data\n",
    "# Load crime data\n",
    "# Load the 2010-2019 crime data\n",
    "crime_df_2010_2019 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", \n",
    "    header=True\n",
    ")\n",
    "\n",
    "# Load the 2020-present crime data\n",
    "crime_df_2020_present = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\", \n",
    "    header=True\n",
    ")\n",
    "crime_df = crime_df_2010_2019.union(crime_df_2020_present)\n",
    "\n",
    "# Clean up the data and classify cases as 'Open' or 'Closed'\n",
    "crime_df = crime_df.withColumn(\n",
    "    \"Case_Status\",\n",
    "    F.when(F.col(\"Status Desc\").isin(\"UNK\", \"Invest Cont\"), \"Open\").otherwise(\"Closed\")\n",
    ")\n",
    "\n",
    "# Create a new column 'Year' based on the 'Date Rptd' timestamp column\n",
    "crime_df = crime_df.withColumn(\"Year\", F.year(F.to_timestamp(\"Date Rptd\", \"MM/dd/yyyy hh:mm:ss a\")))\n",
    "\n",
    "# Register the DataFrame as a temporary SQL view\n",
    "crime_df.createOrReplaceTempView(\"crime_data\")\n",
    "\n",
    "# SQL query to find the top 3 precincts with the highest closed case rates per year\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    Year,\n",
    "    Area,\n",
    "    Closed_Case_Rate,\n",
    "    Rank\n",
    "FROM (\n",
    "    SELECT \n",
    "        Year,\n",
    "        `AREA NAME` AS Area,\n",
    "        (SUM(CASE WHEN Case_Status = 'Closed' THEN 1 ELSE 0 END) / COUNT(*) * 100) AS Closed_Case_Rate,\n",
    "        ROW_NUMBER() OVER (PARTITION BY Year ORDER BY (SUM(CASE WHEN Case_Status = 'Closed' THEN 1 ELSE 0 END) / COUNT(*) * 100) DESC) AS Rank\n",
    "    FROM \n",
    "        crime_data\n",
    "    GROUP BY \n",
    "        Year, `AREA NAME`\n",
    ") AS subquery\n",
    "WHERE Rank <= 3\n",
    "ORDER BY Year, Rank\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "result_df = spark.sql(query)\n",
    "\n",
    "# Show the results\n",
    "result_df.show(truncate=False)\n",
    "\n",
    "\n",
    "# Execute the query\n",
    "result_df = spark.sql(query)\n",
    "\n",
    "# Show the results\n",
    "result_df.show(truncate=False)\n",
    "\n",
    "# Stop timing and print out the execution duration\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Write results to S3 in Parquet format\n",
    "group_number = \"24\"\n",
    "s3_path = \"s3://groups-bucket-dblab-905418150721/group\"+group_number+\"/results/\"\n",
    "\n",
    "result_df.write.mode(\"overwrite\").parquet(s3_path + \"q2_sql_output\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bee830-9b2c-4207-9d06-e7d084e7e739",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
